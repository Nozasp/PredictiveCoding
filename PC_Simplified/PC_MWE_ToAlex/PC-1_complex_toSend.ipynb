{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -*- coding: utf-8 -*-\n",
    "# \"\"\"PC_VERY_Simple_PytorchOptimization_6.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ae': 18.26, 'be': -5.38, 'hme': 78.67, 'ai': 21.97, 'bi': -4.81, 'hmi': 125.62, 'taue': 0.005, 'taui': 0.005, 'tauNMDA': 0.1, 'tauGABA': 0.005, 'tauAMPA': 0.002, 'gamma': 0.641, 'sigma': 0.0007, 'I0e': 0.2346, 'I0i': 0.17, 'sigmaIn': 3, 'sigmaEI': 3, 'sigmaInh': [0.2, 3], 'I_ext': 0.0, 'c_dash': 90, 'mu0': 30, 'Jext': 0.01, 'I1': 0.57, 'I2': 0.029999999999999992, 'Jee': 0.2, 'Jie': 0.2, 'Jei': 1.4, 'Jii': 6.7, 'Jin': 0.008, 'Jiq': 0.85, 'Jes': 3.5, 'Jsi': 0.12, 'Jem': 2.2, 'I_noise': array([[-5.53087287e-05],\n",
      "       [ 8.12621598e-04],\n",
      "       [-4.05608567e-04],\n",
      "       [ 6.76161863e-05]]), 'T': 3, 'dt': 2e-05, 'r_init': 0.2, 'range_t': array([0.00000e+00, 2.00000e-05, 4.00000e-05, ..., 2.99994e+00,\n",
      "       2.99996e+00, 2.99998e+00]), 'Lt': 150000, 'NumN': 20, 'f': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20]), 'In0': 0, 'InMax': 50, 'Iq0': 0, 'IqMax': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # F.mse_loss\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader  # for batch and split Xtrain Ytrain dataset\n",
    "\n",
    "import scipy\n",
    "import scipy.ndimage as nd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from locale import format\n",
    "from dataclasses import dataclass, MISSING\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# from scipy.sparse import identity\n",
    "from icecream import ic  # for debugging. print variable name\n",
    "\n",
    "## !!!!! To get the parameters\n",
    "from PC_Parameters import default_parameters_network\n",
    "\n",
    "pars = default_parameters_network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "### Basics ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2- Input/Output function\n",
    "\n",
    "def plot_io(x, y, sign):\n",
    "    if sign == \"+\":\n",
    "        sign_name = 'Excitatory'\n",
    "        label = \"ae={0}, be={1}, hme={2}\"\n",
    "        a, b, hm = pars['ae'], pars['be'], pars['hme']\n",
    "        color = \"k\"\n",
    "    elif sign == \"-\":\n",
    "        sign_name = 'Inhibitory'\n",
    "        label = \"ai={0}, bi={1}, hmi={2}\"\n",
    "        a, b, hm = pars['ai'], pars['bi'], pars['hmi']\n",
    "        color = \"r\"\n",
    "\n",
    "    plt.plot(x, y, color, label=label.format(a, b, hm))\n",
    "\n",
    "    plt.xlabel(\"Input values - nA\")\n",
    "    plt.ylabel(\"Spike Frequency - Hz\")\n",
    "    plt.xlim([-0.01, 1])\n",
    "    plt.title(\"Input-output function\")\n",
    "    # plt.title(\"{0} Input-output function\".format(sign_name))\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_r(range_sim, r_e, r_i, param, xlim_ar=None):\n",
    "    label_e = \"Excitatoty  Jee={0}, Jei={1}\"  # , I1={2}\"\n",
    "    label_i = \"Inhibitory  Jii={0}, Jie={1}\"  # , I2={2}\"\n",
    "    plt.plot(range_sim, r_e, \"r\", label=label_e.format(param.Jee, param.Jei))  # , param.I1 #, param.In\n",
    "    plt.plot(range_sim, r_i, \"orange\", label=label_i.format(param.Jii, param.Jie))  # , round(param.I2, 2)))\n",
    "\n",
    "    plt.xlabel(\"Time - ms\")\n",
    "    plt.ylabel(\"Spike Frequency - Hz\")\n",
    "    if xlim_ar != None:\n",
    "        plt.xlim(xlim_ar)  # [0, .1]\n",
    "    plt.title(\"Firing rate of the NMDA and GABA populations\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_s(range_sim, S_e, S_i, param, xlim_ar=None):\n",
    "    label_e = \"Excitatoty  Jee={0}, Jei={1}\"  # , I1={2}\"\n",
    "    label_i = \"Inhibitory  Jii={0}, Jie={1}\"  # , I2={2}\"\n",
    "    plt.plot(range_sim, S_e, \"olive\", label=label_e.format(param.Jee, param.Jei))  # , param.I1\n",
    "    plt.plot(range_sim, S_i, \"green\", label=label_i.format(param.Jii, param.Jie))  # , round(param.I2, 2)\n",
    "    if xlim_ar != None:\n",
    "        plt.xlim(xlim_ar)\n",
    "    plt.xlabel(\"Time - ms\")\n",
    "    plt.ylabel(\"Open channel\")\n",
    "    # plt.xlim([0, .1])\n",
    "    plt.title(\"Average open channel for the NMDA and GABA populations\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "# 2- Plot HeatMap of firing rate function\n",
    "def HeatMap(rE, rI, J=None):\n",
    "    if J == None:\n",
    "        J = [.00989, 0.0081, .1, .87, .00081]  # J = dict(Jin=.008, Jee= .2, Jie=.2, Jei=1.4, Jii=6.7)\n",
    "    if type(J) == dict:\n",
    "        J = np.array(list(J.values()))\n",
    "\n",
    "    rE_df = pd.DataFrame(rE.T)  # to get time vs pop\n",
    "    rI_df = pd.DataFrame(rI.T)\n",
    "\n",
    "    rE_df.index.name, rI_df.index.name = [\"Excitatory Population\", \"Inhibitory Population\"]\n",
    "    rE_df.columns.name, rI_df.columns.name = [\"Time s\", \"Time s\"]\n",
    "    # print(rE_df.loc[[10]])\n",
    "\n",
    "    # set context for the upcoming plot\n",
    "    sns.set_context(\"notebook\", font_scale=.8, rc={\"lines.linewidth\": 2.5, 'font.family': 'Helvetica'})\n",
    "\n",
    "    fig, (axA, axB) = plt.subplots(2, 1, figsize=(6, 6))\n",
    "\n",
    "    sns.heatmap(rE_df, ax=axA, cmap=\"viridis\")\n",
    "    sns.heatmap(rI_df, ax=axB)\n",
    "    axA.set_title(f\"Firing rate in Hz of exc populations over time. Jie: {J[2]}, Jee: {J[1]}, Jin: {J[0]}\",\n",
    "                  fontdict={\"fontsize\": 10})\n",
    "    axB.set_title(f\"Firing rate in Hz of inh populations over time. Jei: {J[3]}, Jii: {J[4]}\",\n",
    "                  fontdict={\"fontsize\": 10})\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Filters gauss and Dog and LoG\n",
    "def gaussian_filter(s, N):\n",
    "    k = np.arange(1, N + 1)\n",
    "    n = 1 / (np.sqrt(2 * np.pi) * N * s)\n",
    "    gaussW = n * np.exp(-(k - k[:, np.newaxis]) ** 2 / (2 * s ** 2))\n",
    "    gaussW2 = gaussW / (.009 ** 2 / np.max(gaussW))  # 1\n",
    "    return gaussW2\n",
    "\n",
    "\n",
    "def dog_filter(sOut, N):\n",
    "    sIn = sOut / 30\n",
    "    k = np.arange(1, N + 1)\n",
    "    gaussIn = np.exp(-(k - k[:, np.newaxis]) ** 2 / (2 * sIn ** 2))\n",
    "    gaussOut = np.exp(-(k - k[:, np.newaxis]) ** 2 / (2 * sOut ** 2))\n",
    "    dog = gaussOut - gaussIn\n",
    "    if np.max(dog) == 0 or None:\n",
    "        print('zero max')\n",
    "        dog = 0\n",
    "    else:\n",
    "        dog = dog / (.042 ** 2 / np.max(dog))  # .0088\n",
    "    return dog\n",
    "\n",
    "\n",
    "def LoG_filter(s, N):\n",
    "    x_lap = np.eye(N)\n",
    "    lapl_filter = nd.gaussian_laplace(x_lap, sigma=(s, s))\n",
    "    return lapl_filter\n",
    "\n",
    "\n",
    "def dLogGaus(s=.61, N=20):\n",
    "    dig = LoG_filter(s, N) + gaussian_filter(.019 * s, N)\n",
    "    return dig\n",
    "\n",
    "\n",
    "\"\"\"### Differentiable function for back propagation\n",
    "\n",
    "To avoid non-differentiable araising from discontinuity of the function, I \"relax\" (smoothen) the where() expression by using a sigmoid instead\n",
    "\n",
    "*   if I get : > <SumBackward1 object at 0x7f79da0b9520> # differentiable\n",
    "*   else I get none\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def relu_stim(x, stim):\n",
    "    return torch.nn.functional.relu(1.0 - torch.abs(x - stim),\n",
    "                                    inplace=False)  # inplace = False to avoid implace operation\n",
    "\n",
    "\n",
    "def Dirac(A, N=pars[\"NumN\"]):\n",
    "    y = scipy.signal.unit_impulse(N, idx=(torch.max(torch.argmax(A))))  # , dtype= <class 'float'>)\n",
    "    return torch.tensor(y)\n",
    "\n",
    "\n",
    "\"\"\"### Try Normalization to \"make it proba\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_it_proba(r_e):\n",
    "    sum_r_e = torch.sum(r_e, 1).reshape(r_e.shape[0], 1)\n",
    "    prob_r = torch.div(r_e, sum_r_e)  # torch.transpose(r_e, dim0=0 ,dim1=1) this pose problem\n",
    "    print(prob_r.grad_fn)\n",
    "    prob_r[prob_r != prob_r] = 0.05  # to replace nan to 1/20 - to sum to 1\n",
    "    # print(\"should sum to 1:\", torch.sum(prob_r, 1)) #to check that it worked\n",
    "\n",
    "    return prob_r.reshape(r_e.shape[0], r_e.shape[1])  # log or not log?\n",
    "\n",
    "\n",
    "def log_proba(proba_r):\n",
    "    return torch.log(proba_r)\n",
    "\n",
    "\n",
    "\"\"\"### Try softmax to \"make it proba\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "\"\"\"### Get the expected stimuli : matrix of 1 where stimuli 0 elsewhere\"\"\"\n",
    "\n",
    "\n",
    "# find the stimuli for every X = stim dataset\n",
    "def get_stimuli_input(X_train_tensor):  # input of the shape Xtrain_tensor[5,:,:]\n",
    "    Xargmax = torch.argmax(X_train_tensor, dim=1)\n",
    "    Xmax = torch.max(Xargmax)\n",
    "    return Xmax\n",
    "\n",
    "\n",
    "# replace where function by relu functio which is differentiable\n",
    "def get_expected_Y_relu(X_train_tensor):\n",
    "    x_t = torch.transpose(X_train_tensor, 0, 1)\n",
    "    dirac_2d = torch.zeros(x_t.shape)\n",
    "    stim = get_stimuli_input(\n",
    "        X_train_tensor)  # input of the shape Xtrain_tensor[5,:,:] # here get_stimuli not differenciable\n",
    "\n",
    "    for pop, t in enumerate(x_t):\n",
    "        tpop = torch.tensor(pop)\n",
    "        dirac_2d[pop, :] = torch.nn.functional.relu(1.0 - torch.abs(tpop - stim), inplace=False)\n",
    "    dirac_2d = torch.transpose(dirac_2d, 1, 0)\n",
    "    return dirac_2d\n",
    "\n",
    "\n",
    "def get_expected_Y_relu_1d_where(X_train_tensor):\n",
    "    stim = get_stimuli_input(X_train_tensor)\n",
    "    dirac_1d = torch.zeros(X_train_tensor.shape)\n",
    "    # Calculate the difference between tpop and stim\n",
    "    for pop in enumerate(X_train_tensor):\n",
    "        dirac_1d[pop[0]] = torch.where(pop[0] == torch.tensor(stim), torch.tensor(1.0), torch.tensor(0.0))\n",
    "    return dirac_1d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Basic classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ***************** CLASS ***************************************\n",
    "\n",
    "@dataclass\n",
    "class Parameter:\n",
    "    # °°° Load the parameters °°°\n",
    "\n",
    "    taue: float = pars[\"taue\"]\n",
    "    ae: float = pars['ae']\n",
    "    be, hme, I_noise = pars['be'], pars['hme'], pars['I_noise']\n",
    "    Jee: float = pars['Jee']\n",
    "    taui, ai, bi, hmi = pars['taui'], pars['ai'], pars['bi'], pars['hmi']\n",
    "    Jii: float = pars['Jii']\n",
    "    Jei: float = pars['Jei']\n",
    "    Jie: float = pars['Jie']\n",
    "    Jes, Jsi = pars['Jes'], pars['Jsi']\n",
    "    Jiq: float = pars['Jiq']  # 0.85; #nA\n",
    "    Jin: float = pars['Jin']\n",
    "    tauNMDA, tauAMPA, tauGABA = pars['tauNMDA'], pars['tauAMPA'], pars['tauGABA']\n",
    "    gamma: float = pars['gamma']  # nmda coupling parameter from brunel\n",
    "    # I1, I2 = pars['I1'], pars['I2']\n",
    "    c_dash = pars['c_dash']\n",
    "    sigma = pars['sigma']  # param.sigma = .0007 for Noise\n",
    "    I_noise = pars['sigma'] * np.random.randn(3, 1)\n",
    "    I1 = pars['Jext'] * pars['mu0'] * (1 + pars['c_dash'] / 100)\n",
    "    I2 = pars['Jext'] * pars['mu0'] * (1 - pars['c_dash'] / 100)\n",
    "\n",
    "    sigmaIn = pars['sigmaIn']\n",
    "\n",
    "    # Input parameters\n",
    "    In0 = pars['In0']  # % Spontaneous firing rate of input populations (Hz)\n",
    "    InMax = pars['InMax']  # % Max firing rate of input populations (Hz)\n",
    "    Iq0 = pars['Iq0']  # % Spontaneous firing rate of feedback populations (Hz)\n",
    "    IqMax = pars['IqMax']  # % Max firing rate of feedback populations (Hz)\n",
    "\n",
    "    # Gaussian filter\n",
    "    # sIn = pars['sigmaInh'][0]\n",
    "    # sOut = pars['sigmaInh'][1]\n",
    "\n",
    "    def __init__(self, sEI, sIn, sOut, N):  # sEI=4, sIn=.2, sOut=1.2,\n",
    "        # Weights (from gaussian filter)\n",
    "        self.N = N  # pars['NumN']\n",
    "        self.wei = torch.tensor(dog_filter(sOut, int(N)), dtype=torch.float32)   # .astype( torch.float32))  # , dtype='float64'# fun.dLogGaus(.61, N)  #fun.dog_filter(sIn, sOut, N)#gaussian_filter(sEI, N)\n",
    "        self.wii = torch.tensor(np.eye(int(N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)#np.eye(N) #\n",
    "        self.wie = torch.tensor(gaussian_filter(sEI, int(N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)\n",
    "        self.wes = torch.tensor(np.eye(int(N)), dtype=torch.float32)  #.astype(torch.float32))  # Identity matrix\n",
    "        self.f = np.arange(1, N + 1)\n",
    "        self.sEI = sEI\n",
    "        self.sIn = sIn\n",
    "        self.sOut = sOut\n",
    "\n",
    "    def reset(self):  # https://stackoverflow.com/questions/56878667/setting-default-values-in-a-class\n",
    "\n",
    "        for name, field in self.__dataclass_fields__.items():\n",
    "            if field.default != MISSING:\n",
    "                setattr(self, name, field.default)\n",
    "            else:\n",
    "                setattr(self, name, field.default_factory())\n",
    "\n",
    "\n",
    "# °°° Time of the simulation °°°\n",
    "class Simulation:\n",
    "    def __init__(self, dt, T):\n",
    "        self.dt = dt\n",
    "        self.T = T\n",
    "        self.range_t = (np.arange(0, self.T, self.dt))\n",
    "        self.Lt = self.range_t.size\n",
    "\n",
    "    def printSim(self):\n",
    "        print(\"T time step of the simulation (dt): \", self.dt, \"  Duration of simulation S (T): \", self.T,\n",
    "              \"Length of the time frame (Lt): \", self.Lt)\n",
    "\n",
    "\n",
    "#  °°° Initialisation of the variables °°°\n",
    "class Neurons:\n",
    "    def __init__(neur, Ltime, Ntime, RFfrequency=0, init_r=0, init_S=0):\n",
    "        neur.r = torch.zeros((Ltime, Ntime), dtype=torch.float32, requires_grad=False)\n",
    "        neur.drdt = torch.zeros((Ltime, Ntime), dtype=torch.float32, requires_grad=False)\n",
    "        neur.S = torch.zeros((Ltime, Ntime), dtype=torch.float32, requires_grad=False)\n",
    "        neur.Itot = torch.zeros((Ltime, Ntime), dtype=torch.float32, requires_grad=False)  # , requires_grad = False\n",
    "        neur.Phi = torch.zeros((Ltime, Ntime), dtype=torch.float32, requires_grad=False)  # np.zeros(Ltime)\n",
    "        neur.rinit = init_r\n",
    "        neur.Sinit = init_S\n",
    "        neur.RFf = RFfrequency\n",
    "        if neur.Sinit != 0:\n",
    "            neur.S[0, :] = neur.Sinit\n",
    "        # neur.S = np.full((Ltime, Ntime), neur.Sinit)\n",
    "\n",
    "    def printNeur(neur):\n",
    "        print(\"size S and r andItot and Phi is: \", neur.S.shape, neur.r.shape, neur.Itot.shape, neur.Phi.shape)\n",
    "\n",
    "\n",
    "class Stim:\n",
    "    def __init__(self, param, simu, f, ISI=1, dur=0.2):  # 8 #[10]\n",
    "        self.f = f  # array of frequency stimulus types\n",
    "        self.ISI = ISI  # inter-stimulus interval\n",
    "        self.dur = dur  # duration in s of a specific stimulus segment . The time the frequency fi ll be maintained in the f array\n",
    "        self.tail = 0\n",
    "        self.predDt = 0\n",
    "        self.pred = 0\n",
    "        self.InMax = param.InMax\n",
    "        self.In0 = param.In0\n",
    "\n",
    "        # Instantaneous frequency\n",
    "        f_instant = np.zeros((int(self.ISI / simu.dt) + 1, 1))  # size ISI : 1 /dt : 1000\n",
    "\n",
    "        # print(f.shape)\n",
    "        for fx in self.f:\n",
    "            fx_array = np.concatenate((np.ones((int(self.dur / simu.dt), 1)) * fx,\n",
    "                                       # just 1 frequency of 8 . # inter-stim interval is aslong as stim interval\n",
    "                                       np.zeros((int(self.ISI / simu.dt),\n",
    "                                                 1))))  # so I get 1 list with 1000 lists containing 8 and 1000 lists containing 0\n",
    "        f_stim = np.vstack((f_instant, fx_array))  # stack vertically these arrays # [0] *1000 , [8]*1000, [0]*1000\n",
    "        self.f_stim = f_stim[1:]  # 1400*1\n",
    "\n",
    "    # bottom up sensory Input # duration 1sec\n",
    "    def sensoryInput(self, parameter, simu, sigmaIn=None, paramf=None, f_stim=None, InMax=None, In0=None):\n",
    "        # paramf = np.arange(1, 101)\n",
    "        w = np.exp(-(((paramf or parameter.f) - (f_stim or self.f_stim)) ** 2) / (\n",
    "                2 * (sigmaIn or parameter.sigmaIn) ** 2))  # pars['f'] = 1:N\n",
    "\n",
    "        # totalAct = w.sum(axis = 1) #sum over each row\n",
    "        # norm_w = (w.T / totalAct).T # elementwise division\n",
    "        In = np.where(f_stim or self.f_stim > 0, (InMax or self.InMax) * w + (In0 or self.In0),\n",
    "                      0)  # if stim >0 give InMax * weight + In0 otherwise give 0\n",
    "        if self.tail != 0:\n",
    "            tail_zeros = np.zeros((parameter.N, int(self.tail / simu.dt)))\n",
    "            In = np.hstack((In, tail_zeros))\n",
    "\n",
    "        range_sim = np.arange(1, In.shape[0] + 1)\n",
    "        self.In = In\n",
    "        self.w = w\n",
    "        self.sigmaIn = sigmaIn\n",
    "\n",
    "        return In, range_sim, w, sigmaIn\n",
    "\n",
    "    def printStim(self):\n",
    "        print(\"frequence of stimulus f:\", self.f, \"  ISI:\", self.ISI, \" Size In:\", self.In.shape, \"Size w:\",\n",
    "              self.w.shape, \"  f_stim:\", self.f_stim.shape,\n",
    "              \"sigmaIn:\", self.sigmaIn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module): #nn.Sequential\n",
    "    def __init__(self, param, sim, Jall): #,**kwargs): ## Chang !! only input in init is self, N is the only parameter\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define other model parameters, layers, or components here if needed\n",
    "        self.param = param\n",
    "        self.dt = sim.dt #hardcode here as well\n",
    "        self.N = (param.N) #hardcode here as well\n",
    "        self.Jee = nn.Parameter(torch.tensor(0.072, requires_grad = True))#just the number here )Jall[0])\n",
    "        self.Jei = nn.Parameter(Jall[1])\n",
    "        self.Jie = nn.Parameter(Jall[2])\n",
    "        self.Jii = nn.Parameter(Jall[3])\n",
    "        self.Jin = nn.Parameter(Jall[4])\n",
    "\n",
    "        # remove phi\n",
    "    def phi(self, I_tot, a, b, hm): #)))  # this use a lot of memory - exponential part\n",
    "        \"\"\"neg_mulA_I = torch.neg(torch.multiply(a, I_tot))\n",
    "        exp_b = torch.exp(torch.add(neg_mulA_I, b))\n",
    "        divide_and_one = torch.divide(1, torch.add(exp_b, 1))\n",
    "        mul_hm = torch.mul(hm, divide_and_one) #mul_hm\"\"\"\n",
    "        return torch.multiply(hm, torch.divide(1, (1 + torch.exp(- (torch.multiply(a, I_tot) + b)))))  #torch.multiply(hm, torch.divide(torch.ones(1), (torch.add(torch.ones(1), torch.exp(torch.add(torch.neg(torch.multiply(a, I_tot)), b)))\n",
    "            \n",
    "\n",
    "    def forward(self, In):\n",
    "        #In = stim\n",
    "        # Initialize model variables here\n",
    "        r_e = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)  \n",
    "        r_i = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)\n",
    "        s_ampa = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)#.detach()\n",
    "        s_gaba = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)\n",
    "        i_tot_e = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)\n",
    "        i_tot_i = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)\n",
    "        dr_e_dt = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)\n",
    "        dr_i_dt = torch.zeros((In.shape[0], self.N), requires_grad=False, dtype=torch.float32)\n",
    "        \n",
    "        # Perform integration over time steps \n",
    "        # Not storing all the time step!\n",
    "        for k in range(1, In.shape[0]):\n",
    "            s_gaba_wie = s_gaba[k - 1, :] @ self.param.wie  # replaced torch.matmul() by @\n",
    "            s_ampa_wei = s_ampa[k - 1, :] @ self.param.wei  # @ is an inplace operation!\n",
    "            s_gaba_wii = s_gaba[k - 1, :] @ (self.param.wii)\n",
    "            \n",
    "            # ic(Sampa_Jee.grad_fn)\n",
    "            ##make Jee *r depend on r # \n",
    "            i_tot_e[k, :] = self.Jee * s_ampa[k - 1, :] - (self.Jie * s_gaba_wie) + self.Jin * In[k - 1, :]\n",
    "            i_tot_i[k, :] = (self.Jei * s_ampa_wei) - (self.Jii * s_gaba_wii)\n",
    "\n",
    "\n",
    "            phi_arr_e = self.phi(i_tot_e[k, :], self.param.ae, self.param.be, self.param.hme)\n",
    "            phi_arr_i = self.phi(i_tot_i[k, :], self.param.ai, self.param.bi, self.param.hmi)\n",
    "\n",
    "            dr_e_dt[k, :] = (-r_e[k - 1, :] + phi_arr_e) / self.param.taue#Jee*r remove pi and S\n",
    "            dr_i_dt[k, :] = (-r_i[k - 1, :] + phi_arr_i) / self.param.taui\n",
    "\n",
    "            r_e[k, :] = r_e[k - 1, :] + dr_e_dt[k, :] * self.dt\n",
    "            r_i[k, :] = r_i[k - 1, :] + dr_i_dt[k, :] * self.dt\n",
    "\n",
    "            dS_amp_dt = (- s_ampa[k - 1, :] / self.param.tauAMPA) + r_e[k, :]\n",
    "            s_ampa[k, :] = s_ampa[k - 1, :] + dS_amp_dt * self.dt\n",
    "\n",
    "            dS_gab_dt = (- s_gaba[k - 1, :] / self.param.tauGABA) + r_i[k, :]\n",
    "            s_gaba[k, :] = s_gaba[k - 1, :] + dS_gab_dt * self.dt\n",
    "\n",
    "        return softmax(r_e), softmax(r_i) #softmax softmax  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#sgaba = s_gaba.detach().clone()\\n#s_gaba_wie = s_gaba[k - 1, :] @ self.param.wie  # replaced torch.matmul() by @\\ns_gaba_wie = torch.matmul(s_gaba[k - 1, :], self.param.wie)\\n#sampa = s_ampa.detach().clone()\\n#s_ampa_wei = s_ampa[k - 1, :] @ self.param.wei\\ns_ampa_wei = torch.matmul(s_ampa[k - 1, :], self.param.wei) # @ is an inplace operation!\\n#s_gaba_wii = s_gaba[k - 1, :] @ (self.param.wii)\\ns_gaba_wii = torch.matmul(s_gaba[k - 1, :], self.param.wii)\\n#s_ampa = s_ampa.detach().clone()\\nsampa = s_ampa[k - 1, :]# .detach().clone()\\nSampa_Jee = torch.mul(self.Jee, sampa)\\nSgaba_Jie = torch.mul(self.Jie, s_gaba_wie)\\nIn_Jin = torch.mul(self.Jin, In[k - 1, :])# self.Jin * In[k - 1, :] # mul for scalar\\nSampa_Jei = torch.mul(self.Jei, s_ampa_wei) #self.Jee * s_ampa_wei#torch.matmul() # self.Jee.clone() destroys the gradient\\n# ic(Sampa_Jee.grad_fn)\\ni_tot_e[k, :] = Sampa_Jee - Sgaba_Jie + In_Jin\\ni_tot_i[k, :] = Sampa_Jei - torch.mul(self.Jii, s_gaba_wii)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self.Jee = nn.Parameter((torch.tensor((self.dicJ['Jee']), requires_grad=True, dtype=torch.float32)))\n",
    "self.Jei = nn.Parameter((torch.tensor((self.dicJ['Jei']), requires_grad=True, dtype=torch.float32)))\n",
    "self.Jie = nn.Parameter((torch.tensor((self.dicJ['Jie']), requires_grad=True, dtype=torch.float32)))\n",
    "self.Jii = nn.Parameter((torch.tensor((self.dicJ['Jii']), requires_grad=True, dtype=torch.float32)))\n",
    "self.Jin = nn.Parameter((torch.tensor((self.dicJ['Jin']), requires_grad=True, dtype=torch.float32)))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#sgaba = s_gaba.detach().clone()\n",
    "#s_gaba_wie = s_gaba[k - 1, :] @ self.param.wie  # replaced torch.matmul() by @\n",
    "s_gaba_wie = torch.matmul(s_gaba[k - 1, :], self.param.wie)\n",
    "#sampa = s_ampa.detach().clone()\n",
    "#s_ampa_wei = s_ampa[k - 1, :] @ self.param.wei\n",
    "s_ampa_wei = torch.matmul(s_ampa[k - 1, :], self.param.wei) # @ is an inplace operation!\n",
    "#s_gaba_wii = s_gaba[k - 1, :] @ (self.param.wii)\n",
    "s_gaba_wii = torch.matmul(s_gaba[k - 1, :], self.param.wii)\n",
    "#s_ampa = s_ampa.detach().clone()\n",
    "sampa = s_ampa[k - 1, :]# .detach().clone()\n",
    "Sampa_Jee = torch.mul(self.Jee, sampa)\n",
    "Sgaba_Jie = torch.mul(self.Jie, s_gaba_wie)\n",
    "In_Jin = torch.mul(self.Jin, In[k - 1, :])# self.Jin * In[k - 1, :] # mul for scalar\n",
    "Sampa_Jei = torch.mul(self.Jei, s_ampa_wei) #self.Jee * s_ampa_wei#torch.matmul() # self.Jee.clone() destroys the gradient\n",
    "# ic(Sampa_Jee.grad_fn)\n",
    "i_tot_e[k, :] = Sampa_Jee - Sgaba_Jie + In_Jin\n",
    "i_tot_i[k, :] = Sampa_Jei - torch.mul(self.Jii, s_gaba_wii)\"\"\"\n",
    "\n",
    "#i_tot_e[k, :] = (self.Jee * s_ampa[k-1,:]) - (self.Jie * s_gaba_wie) + (self.Jin * In[k - 1, :])\n",
    "#i_tot_i[k, :] = (self.Jei * s_ampa_wei) - (self.Jii * s_gaba_wii)\n",
    "\n",
    "#Phi\n",
    "# return hm * (1 / (1 + torch.exp(- (a * I_tot + b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the model and the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sIN 0.1 sout 3.0 sEI 0.2\n",
      "Jee: 0.2   Jei: 1.4\n",
      "Jii 6.7 Jie: 0.2  Jin: 0.008\n",
      "T time step of the simulation (dt):  0.0001   Duration of simulation S (T):  0.4 Length of the time frame (Lt):  4000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = 20\n",
    "# \\\\\\\\\\\\\\\\\\\\\\ Parameters\n",
    "param = Parameter(N=20, sIn=.1, sOut=3., sEI=.2)\n",
    "sigmas = [param.sOut, param.sEI]  # = sigmas\n",
    "print(\"sIN\", sigmas[0] / 30, \"sout\", sigmas[0], \"sEI\", sigmas[1])\n",
    "print(\"Jee:\", param.Jee, \"  Jei:\", param.Jei)\n",
    "print(\"Jii\", param.Jii, \"Jie:\", param.Jie, \" Jin:\", param.Jin)\n",
    "J = param.Jin, param.Jee, param.Jie, param.Jei, param.Jii\n",
    "\n",
    "# \\\\\\\\\\\\\\\\\\\\\\ Simulation time\n",
    "simu = Simulation(1e-4, .4)  # dt #rangeSim #dur = 2s\n",
    "simu.printSim()\n",
    "\n",
    "# \\\\\\\\\\\\\\\\\\\\\\ Bottom up sensory input\n",
    "stimuli = Stim(param, simu, dur=.3, f=[8], ISI=.05)  # dur = 1s Isi=1s\n",
    "In, range_sim, w, sigmaIn = stimuli.sensoryInput(param, simu, sigmaIn=2.)\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++ Initialize the Model ++++++++++++++++++++++++++++\n",
    "J1 = {'Jee': 0.072, 'Jei': 0.004, 'Jie': 0.05, 'Jii': 0.6, 'Jin': 0.00695}\n",
    "Jee = torch.tensor((J1['Jee']), requires_grad=True, dtype=torch.float32)\n",
    "Jei = ((torch.tensor((J1['Jei']), requires_grad=True, dtype=torch.float32)))\n",
    "Jie = ((torch.tensor((J1['Jie']), requires_grad=True, dtype=torch.float32)))\n",
    "Jii = ((torch.tensor((J1['Jii']), requires_grad=True, dtype=torch.float32)))\n",
    "Jin = ((torch.tensor((J1['Jin']), requires_grad=True, dtype=torch.float32)))\n",
    "\n",
    "Jall = [Jee,Jei,Jie,Jii,Jin]       \n",
    "mymodel = MyModel(param, simu, Jall)#**J1)  # these ** pass J as an unpacked dict, not as a single positional argument\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN forward pass and Print heatmap ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "sti = torch.tensor(stimuli.In, dtype=torch.float32) #).float()\n",
    "#r_e, r_i = mymodel.forward(stim=sti)  # ,r_i, dr_e_dt, dr_i_dt, s_ampa, s_gaba, i_tot_e, i_tot_i, In,\n",
    "\n",
    "#HeatMap(r_e.detach().numpy(), r_i.detach().numpy(), J1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# +++++++++++++++++++++++++ Optimizer ++++++++++++++++++++++++++++\n",
    "learning_rate = 0.001 #0.00001 #0,001\n",
    "optimizer = optim.SGD(mymodel.parameters(),\n",
    "                      lr=learning_rate) \n",
    "\n",
    "# +++++++++++++++++++++++++ Epochs +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "num_epochs = 2\n",
    "\n",
    "# +++++++++++++++++++++++++ Inputs + Labels +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "J1 = {'Jee': 0.072, 'Jei': 0.004, 'Jie': 0.05, 'Jii': 0.6, 'Jin': 0.00695}  #J_tensor = torch.tensor([0.072, 0.004, 0.05, 0.6, 0.00695])\n",
    "J_list = list(J1.keys())\n",
    "\n",
    "X_input = sti.requires_grad_(False)  # .requires_grad_(True)\n",
    "Y_target = get_expected_Y_relu(X_input)  # .requires_grad_(False)\n",
    "losses = torch.zeros(num_epochs)\n",
    "\n",
    "#Y_prediction, r_i = mymodel.forward(stim=sti)\n",
    "#HeatMap(X_input.detach().numpy(), r_i.detach().numpy(), J1)\n",
    "#HeatMap(Y_prediction.detach().numpy(), r_i.detach().numpy(), J1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3046, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3101, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3488, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\knzga\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Local\\Temp\\ipykernel_15908\\1203903348.py\", line 36, in <module>\n",
      "    Y_prediction, I = mymodel(X_input) #, I  # make output proba #Y_prediction = softmax(r_e)  # torch.exp(r_e) / torch.sum(torch.exp(r_e), axis=1))\n",
      "  File \"c:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\knzga\\AppData\\Local\\Temp\\ipykernel_15908\\954701323.py\", line 48, in forward\n",
      "    i_tot_e[k, :] = self.Jee * s_ampa[k - 1, :] - (self.Jie * s_gaba_wie) + self.Jin * In[k - 1, :]\n",
      " (Triggered internally at ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [20]], which is output 0 of AsStridedBackward0, is at version 3998; expected version 3997 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\knzga\\Documents\\02_Computational Neuroscience Project\\Programming\\VScode\\PredictiveCoding\\PC_Simplified\\PC-1.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mmean((Y_prediction \u001b[39m-\u001b[39m Y_target)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))  \u001b[39m# sum first for every population  at each time step, then sum over each time step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m#ic(loss.grad_fn)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\u001b[39m#retain_graph=True\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Compute gradients for the loss with respect to specific tensors of interest #gradient of the loss only???\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#grads = torch.autograd.grad(loss, list(mymodel.parameters())) # # loss.backward()#retain_graph=True\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Update the parameters using the computed gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m#for param, grad in zip(list(mymodel.parameters()), grads):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m#print(grads, grad, param)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m#param.grad = grad\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/knzga/Documents/02_Computational%20Neuroscience%20Project/Programming/VScode/PredictiveCoding/PC_Simplified/PC-1.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\knzga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [20]], which is output 0 of AsStridedBackward0, is at version 3998; expected version 3997 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# +++++++++++++++++++++++++ Problems investigations +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# import tracemalloc\n",
    "# tracemalloc.start()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def zero_grad2(params):\n",
    "    \"\"\"\n",
    "  Clear gradients as they accumulate on successive backward calls\n",
    "  Args:\n",
    "    params: an iterator over tensors\n",
    "  \"\"\"\n",
    "    for par in params:\n",
    "        if not (par.grad is None):\n",
    "            par.grad.data.zero_()\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++ Optimization loop +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        \n",
    "        # Clear the gradients and zero_grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate output\n",
    "        Y_prediction, I = mymodel(X_input) \n",
    "\n",
    "        # calculate loss\n",
    "        loss = torch.sum(torch.mean((Y_prediction - Y_target)**2, axis=1))  \n",
    "        #ic(loss.grad_fn)\n",
    "        \n",
    "        loss.backward()#retain_graph=True\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #losses[epoch] = loss\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss}, Loss_grad: {loss.grad_fn}')  # .item()\n",
    "        #ic((mymodel.parameters()))\n",
    "        for i, par in enumerate(mymodel.parameters()):\n",
    "            ic(J_list[i], par, par.grad)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## +++++++++++++++++++++++++ Loss over time +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "plt.plot(np.arange(losses.shape[0]), losses.detach().numpy(), 'bo', label='Training loss')\n",
    "plt.title(f\"Loss over {num_epochs} epochs for a learning rate of {learning_rate}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
