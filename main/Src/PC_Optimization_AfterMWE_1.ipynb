{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgKrvSpapxwW",
        "outputId": "74f939aa-fe70-4665-f50c-016ab6029296"
      },
      "outputs": [],
      "source": [
        "#!pip install torchviz\n",
        "#!pip install icecream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrR17p1ipoFd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DQp7y2sppSr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # F.mse_loss\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader  # for batch and split Xtrain Ytrain dataset\n",
        "import sys\n",
        "import torchviz\n",
        "import scipy\n",
        "import scipy.ndimage as nd\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "from locale import format\n",
        "from dataclasses import dataclass, MISSING\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "from scipy.interpolate import griddata\n",
        "from scipy import special\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import math\n",
        "# from scipy.sparse import identity\n",
        "from icecream import ic  # for debugging. print variable name\n",
        "\n",
        "## !!!!! To get the parameters\n",
        "#from gdrive.MyDrive.Github.PredictiveCoding.main.Src.PC_param import default_parameters_network\n",
        "\n",
        "#from PC_param import default_parameters_network\n",
        "#import PC_param.ipynb\n",
        "#from PC_Parameters import default_parameters_network\n",
        "\n",
        "## !!!!! To get the parameters\n",
        "import sys\n",
        "#sys.path.insert(1, '/Users/knzga/Documents/folder/02_Computational Neuroscience Project/Programming/VScode/PredictiveCoding/main/Src/PC_param.ipynb')\n",
        "sys.path.insert(1, '/main/CustomPackages/PC_param.ipynb')\n",
        "#https://github.com/Nozasp/PredictiveCoding/blob/main/\n",
        "#main/Src\n",
        "\n",
        "#C:\\Users\\knzga\\Documents\\02_Computational Neuroscience Project\\Programming\\VScode\\PredictiveCoding\\main\\Src> \n",
        "#from PC_param import default_parameters_network #from PC_param \n",
        "from PC_Parameters import default_parameters_network\n",
        "\n",
        "pars = default_parameters_network()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dht5lPGapvVL"
      },
      "source": [
        "# Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtML8XGSptv2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### 2- Input/Output function\n",
        "\n",
        "def plot_io(x, y, sign):\n",
        "    if sign == \"+\":\n",
        "        sign_name = 'Excitatory'\n",
        "        label = \"ae={0}, be={1}, hme={2}\"\n",
        "        a, b, hm = pars['ae'], pars['be'], pars['hme']\n",
        "        color = \"k\"\n",
        "    elif sign == \"-\":\n",
        "        sign_name = 'Inhibitory'\n",
        "        label = \"ai={0}, bi={1}, hmi={2}\"\n",
        "        a, b, hm = pars['ai'], pars['bi'], pars['hmi']\n",
        "        color = \"r\"\n",
        "\n",
        "    plt.plot(x, y, color, label=label.format(a, b, hm))\n",
        "\n",
        "    plt.xlabel(\"Input values - nA\")\n",
        "    plt.ylabel(\"Spike Frequency - Hz\")\n",
        "    plt.xlim([-0.01, 1])\n",
        "    plt.title(\"Input-output function\")\n",
        "    # plt.title(\"{0} Input-output function\".format(sign_name))\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_r(range_sim, r_e, r_i, param, xlim_ar=None):\n",
        "    label_e = \"Excitatoty  Jee={0}, Jei={1}\"  # , I1={2}\"\n",
        "    label_i = \"Inhibitory  Jii={0}, Jie={1}\"  # , I2={2}\"\n",
        "    plt.plot(range_sim, r_e, \"r\", label=label_e.format(param.Jee, param.Jei))  # , param.I1 #, param.In\n",
        "    plt.plot(range_sim, r_i, \"orange\", label=label_i.format(param.Jii, param.Jie))  # , round(param.I2, 2)))\n",
        "\n",
        "    plt.xlabel(\"Time - ms\")\n",
        "    plt.ylabel(\"Spike Frequency - Hz\")\n",
        "    if xlim_ar != None:\n",
        "        plt.xlim(xlim_ar)  # [0, .1]\n",
        "    plt.title(\"Firing rate of the NMDA and GABA populations\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_s(range_sim, S_e, S_i, param, xlim_ar=None):\n",
        "    label_e = \"Excitatoty  Jee={0}, Jei={1}\"  # , I1={2}\"\n",
        "    label_i = \"Inhibitory  Jii={0}, Jie={1}\"  # , I2={2}\"\n",
        "    plt.plot(range_sim, S_e, \"olive\", label=label_e.format(param.Jee, param.Jei))  # , param.I1\n",
        "    plt.plot(range_sim, S_i, \"green\", label=label_i.format(param.Jii, param.Jie))  # , round(param.I2, 2)\n",
        "    if xlim_ar != None:\n",
        "        plt.xlim(xlim_ar)\n",
        "    plt.xlabel(\"Time - ms\")\n",
        "    plt.ylabel(\"Open channel\")\n",
        "    # plt.xlim([0, .1])\n",
        "    plt.title(\"Average open channel for the NMDA and GABA populations\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "# 2- Plot HeatMap of firing rate function\n",
        "def HeatMap(rE, rI, J=None):\n",
        "    if J == None:\n",
        "        J = [.00989, 0.0081, .1, .87, .00081]  # J = dict(Jin=.008, Jee= .2, Jie=.2, Jei=1.4, Jii=6.7)\n",
        "    if type(J) == dict:\n",
        "        J = np.array(list(J.values()))\n",
        "\n",
        "    rE_df = pd.DataFrame(rE.T)  # to get time vs pop\n",
        "    rI_df = pd.DataFrame(rI.T)\n",
        "    rE_df.index = rE_df.index + 1\n",
        "    rI_df.index = rI_df.index + 1\n",
        "    rE_df.index.name, rI_df.index.name = [\"Excitatory Population\", \"Inhibitory Population\"]\n",
        "    rE_df.columns.name, rI_df.columns.name = [\"Time s\", \"Time s\"]\n",
        "    # print(rE_df.loc[[10]])\n",
        "\n",
        "    # set context for the upcoming plot\n",
        "    sns.set_context(\"notebook\", font_scale=.8, rc={\"lines.linewidth\": 2.5, 'font.family': 'Helvetica'})\n",
        "\n",
        "    fig, (axA, axB) = plt.subplots(2, 1, figsize=(6, 6))\n",
        "\n",
        "    sns.heatmap(rE_df, ax=axA, cmap=\"viridis\")\n",
        "    sns.heatmap(rI_df, ax=axB)\n",
        "    axA.set_title(f\"Firing rate in Hz of exc populations over time. Jie: {J[2]}, Jee: {J[1]}, Jin: {J[0]}\",\n",
        "                  fontdict={\"fontsize\": 10})\n",
        "    axB.set_title(f\"Firing rate in Hz of inh populations over time. Jei: {J[3]}, Jii: {J[4]}\",\n",
        "                  fontdict={\"fontsize\": 10})\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Filters gauss and Dog and LoG\n",
        "def gaussian_filter(s, N):\n",
        "    k = np.arange(1, N + 1)\n",
        "    n = 1 / (np.sqrt(2 * np.pi) * N * s)\n",
        "    gaussW = n * np.exp(-(k - k[:, np.newaxis]) ** 2 / (2 * s ** 2))\n",
        "    gaussW2 = gaussW / (.009 ** 2 / np.max(gaussW))  # 1\n",
        "    return gaussW2\n",
        "\n",
        "\n",
        "def dog_filter(sOut, N):\n",
        "    sIn = sOut / 30\n",
        "    k = np.arange(1, N + 1)\n",
        "    gaussIn = np.exp(-(k - k[:, np.newaxis]) ** 2 / (2 * sIn ** 2))\n",
        "    gaussOut = np.exp(-(k - k[:, np.newaxis]) ** 2 / (2 * sOut ** 2))\n",
        "    dog = gaussOut - gaussIn\n",
        "    if np.max(dog) == 0 or None:\n",
        "        print('zero max')\n",
        "        dog = 0\n",
        "    else:\n",
        "        dog = dog / (.042 ** 2 / np.max(dog))  # .0088\n",
        "    return dog\n",
        "\n",
        "\n",
        "def LoG_filter(s, N):\n",
        "    x_lap = np.eye(N)\n",
        "    lapl_filter = nd.gaussian_laplace(x_lap, sigma=(s, s))\n",
        "    return lapl_filter\n",
        "\n",
        "\n",
        "def dLogGaus(s=.61, N=20):\n",
        "    dig = LoG_filter(s, N) + gaussian_filter(.019 * s, N)\n",
        "    return dig\n",
        "\n",
        "\n",
        "\"\"\"### Differentiable function for back propagation\n",
        "\n",
        "To avoid non-differentiable araising from discontinuity of the function, I \"relax\" (smoothen) the where() expression by using a sigmoid instead\n",
        "*   with grad_fn:\n",
        "*   if I get : > <SumBackward1 object at 0x7f79da0b9520> # differentiable\n",
        "*   else I get none\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def relu_stim(x, stim):\n",
        "    return torch.nn.functional.relu(1.0 - torch.abs(x - stim),\n",
        "                                    inplace=False)  # inplace = False to avoid implace operation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Dirac(A, N=pars[\"NumN\"]):\n",
        "    y = scipy.signal.unit_impulse(N, idx=(torch.max(torch.argmax(A))))  # , dtype= <class 'float'>)\n",
        "    return torch.tensor(y)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"### Try Normalization to \"make it proba\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_it_proba(r_e):\n",
        "    \"\"\"sum_r_e = torch.sum(r_e, 1).reshape(r_e.shape[0], 1)\n",
        "    prob_r = torch.div(r_e, sum_r_e)\n",
        "    print(prob_r.grad_fn)\n",
        "    prob_r[prob_r != prob_r] = 0.05\"\"\"  # to replace nan to 1/20 - to sum to 1\n",
        "    # print(\"should sum to 1:\", torch.sum(prob_r, 1)) #to check that it worked\n",
        "    baseline = 1\n",
        "    sum_r_e_and_baseline = torch.sum(r_e, 1).reshape(r_e.shape[0], 1) + baseline\n",
        "    prob_r = torch.div(r_e + baseline, sum_r_e_and_baseline)\n",
        "\n",
        "    return prob_r.reshape(r_e.shape[0], r_e.shape[1])  # log or not log?\n",
        "\n",
        "\n",
        "def make_it_proba_1d(r_e):\n",
        "    baseline = 1\n",
        "    sum_r_e = torch.sum(r_e) + baseline\n",
        "    prob_r = torch.div(r_e +baseline, sum_r_e)  # torch.transpose(r_e, dim0=0 ,dim1=1) poses a problem\n",
        "    #prob_r[prob_r != prob_r] = 0.05  # to replace nan to 1/20 - to sum to 1\n",
        "    print(\"should sum to 1:\", torch.sum(prob_r)) #to check that it worked\n",
        "\n",
        "    return prob_r\n",
        "\n",
        "\n",
        "def log_proba(proba_r):\n",
        "    return torch.log(proba_r)\n",
        "\n",
        "\n",
        "\"\"\"### Try softmax to \"make it proba\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    return torch.exp(x) / torch.sum(torch.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "def softmax1D(x):\n",
        "    return torch.exp(x) / torch.sum(torch.exp(x))\n",
        "\n",
        "\"\"\"### Get the expected stimuli : matrix of 1 where stimuli 0 elsewhere\"\"\"\n",
        "\n",
        "\n",
        "# find the stimuli for every X = stim dataset\n",
        "# find the stimuli for every X = stim dataset\n",
        "def get_stimuli_input(X_train_tensor):  # input of the shape Xtrain_tensor[5,:,:]\n",
        "    Xargmax = torch.argmax(X_train_tensor, dim=1)\n",
        "    Xmax = torch.max(Xargmax)\n",
        "    return Xmax\n",
        "\n",
        "def get_stimuli_input1D(X_train_tensor):  # input of the shape Xtrain_tensor[5,:,:]\n",
        "    Xargmax = torch.argmax(X_train_tensor)#, dim=1)\n",
        "    Xmax = torch.max(Xargmax)\n",
        "    return Xmax\n",
        "\n",
        "\n",
        "# replace where function by relu functio which is differentiable\n",
        "def get_expected_Y_relu(X_train_tensor):\n",
        "    x_t = torch.transpose(X_train_tensor, 0, 1)\n",
        "    dirac_2d = torch.zeros(x_t.shape)\n",
        "    stim = get_stimuli_input(\n",
        "        X_train_tensor)  # input of the shape Xtrain_tensor[5,:,:] # here get_stimuli not differenciable\n",
        "\n",
        "    for pop, t in enumerate(x_t):\n",
        "        tpop = torch.tensor(pop)\n",
        "        dirac_2d[pop, :] = torch.nn.functional.relu(1.0 - torch.abs(tpop - stim), inplace=False).requires_grad_(False)\n",
        "    dirac_2d = torch.transpose(dirac_2d, 1, 0)\n",
        "    return dirac_2d\n",
        "\n",
        "\n",
        "def get_expected_Y_relu_1d_where(X_train_tensor):\n",
        "    stim = get_stimuli_input1D(X_train_tensor)\n",
        "    dirac_1d = torch.zeros(X_train_tensor.shape)\n",
        "    # Calculate the difference between tpop and stim\n",
        "    for pop in enumerate(X_train_tensor):\n",
        "        dirac_1d[pop[0]] = torch.where(pop[0] == torch.tensor(stim), torch.tensor(1.0), torch.tensor(0.0)).requires_grad_(False)#true #not differenciable\n",
        "    return dirac_1d\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3oROWSKFJPa"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0DyLbiAFKpI"
      },
      "source": [
        "## Simple Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIQe8vT3FML_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "*\n",
        "*\n",
        "***********  CLASS\n",
        "*\n",
        "*\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ***************** CLASS ***************************************\n",
        "\n",
        "@dataclass\n",
        "class Parameter:\n",
        "    # °°° Load the parameters °°°\n",
        "\n",
        "    taue: float = pars[\"taue\"]\n",
        "    ae: float = pars['ae']\n",
        "    be, hme, I_noise = pars['be'], pars['hme'], pars['I_noise']\n",
        "    Jee: float = pars['Jee']\n",
        "    taui, ai, bi, hmi = pars['taui'], pars['ai'], pars['bi'], pars['hmi']\n",
        "    Jii: float = pars['Jii']\n",
        "    Jei: float = pars['Jei']\n",
        "    Jie: float = pars['Jie']\n",
        "    Jes, Jsi = pars['Jes'], pars['Jsi']\n",
        "    Jiq: float = pars['Jiq']  # 0.85; #nA\n",
        "    Jin: float = pars['Jin']\n",
        "    tauNMDA, tauAMPA, tauGABA = pars['tauNMDA'], pars['tauAMPA'], pars['tauGABA']\n",
        "    gamma: float = pars['gamma']  # nmda coupling parameter from brunel\n",
        "    c_dash = pars['c_dash']\n",
        "    sigma = pars['sigma']  # param.sigma = .0007 for Noise\n",
        "    I_noise = pars['sigma'] * np.random.randn(3, 1)\n",
        "    I1 = pars['Jext'] * pars['mu0'] * (1 + pars['c_dash'] / 100)\n",
        "    I2 = pars['Jext'] * pars['mu0'] * (1 - pars['c_dash'] / 100)\n",
        "    # I1, I2 = pars['I1'], pars['I2']\n",
        "\n",
        "    sigmaIn = pars['sigmaIn']\n",
        "\n",
        "    # Input parameters\n",
        "    In0 = pars['In0']  # % Spontaneous firing rate of input populations (Hz)\n",
        "    InMax = pars['InMax']  # % Max firing rate of input populations (Hz)\n",
        "    Iq0 = pars['Iq0']  # % Spontaneous firing rate of feedback populations (Hz)\n",
        "    IqMax = pars['IqMax']  # % Max firing rate of feedback populations (Hz)\n",
        "\n",
        "    # Gaussian filter\n",
        "    # sIn = pars['sigmaInh'][0]\n",
        "    # sOut = pars['sigmaInh'][1]\n",
        "\n",
        "    def __init__(self, sEI, sIn, sOut, N):  # sEI=4, sIn=.2, sOut=1.2,\n",
        "        # Weights (from gaussian filter)\n",
        "        self.N = N  # pars['NumN']\n",
        "        self.wei = torch.tensor(dog_filter(sOut, int(N)), dtype=torch.float32)   # .astype( torch.float32))  # , dtype='float64'# fun.dLogGaus(.61, N)  #fun.dog_filter(sIn, sOut, N)#gaussian_filter(sEI, N)\n",
        "        self.wii = torch.tensor(np.eye(int(N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)#np.eye(N) #\n",
        "        self.wie = torch.tensor(gaussian_filter(sEI, int(N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)\n",
        "        self.wes = torch.tensor(np.eye(int(N)), dtype=torch.float32)  #.astype(torch.float32))  # Identity matrix\n",
        "        self.f = np.arange(1, N + 1)\n",
        "        self.sEI = sEI\n",
        "        self.sIn = sIn\n",
        "        self.sOut = sOut\n",
        "\n",
        "    def reset(self):  # https://stackoverflow.com/questions/56878667/setting-default-values-in-a-class\n",
        "\n",
        "        for name, field in self.__dataclass_fields__.items():\n",
        "            if field.default != MISSING:\n",
        "                setattr(self, name, field.default)\n",
        "            else:\n",
        "                setattr(self, name, field.default_factory())\n",
        "\n",
        "\n",
        "# °°° Time of the simulation °°°\n",
        "class Simulation:\n",
        "    def __init__(self, dt, T):\n",
        "        self.dt = dt\n",
        "        self.T = T\n",
        "        self.range_t = (np.arange(0, self.T, self.dt))\n",
        "        self.Lt = self.range_t.size\n",
        "\n",
        "    def printSim(self):\n",
        "        print(\"T time step of the simulation (dt): \", self.dt, \"  Duration of simulation S (T): \", self.T,\n",
        "              \"Length of the time frame (Lt): \", self.Lt)\n",
        "\n",
        "\n",
        "#  °°° Initialisation of the variables °°°\n",
        "\n",
        "class Stim:\n",
        "    def __init__(self, param, simu, f, ISI=1, dur=0.2):  # 8 #[10]\n",
        "        self.f = f  # array of frequency stimulus types\n",
        "        self.ISI = ISI  # inter-stimulus interval\n",
        "        self.dur = dur  # duration in s of a specific stimulus segment . The time the frequency fi ll be maintained in the f array\n",
        "        self.tail = 0\n",
        "        self.predDt = 0\n",
        "        self.pred = 0\n",
        "        self.InMax = param.InMax\n",
        "        self.In0 = param.In0\n",
        "\n",
        "        # Instantaneous frequency\n",
        "        f_instant = np.zeros((int(self.ISI / simu.dt) + 1, 1))  # size ISI : 1 /dt : 1000\n",
        "\n",
        "        for fx in self.f:\n",
        "            fx_array = np.concatenate((np.ones((int(self.dur / simu.dt), 1)) * fx,\n",
        "                                       # just 1 frequency of 8 . # inter-stim interval is aslong as stim interval\n",
        "                                       np.zeros((int(self.ISI / simu.dt),\n",
        "                                                 1))))  # so I get 1 list with 1000 lists containing 8 and 1000 lists containing 0\n",
        "        f_stim = np.vstack((f_instant, fx_array))  # stack vertically these arrays # [0] *1000 , [8]*1000, [0]*1000\n",
        "        self.f_stim = f_stim[1:]  # 1400*1\n",
        "\n",
        "    # bottom up sensory Input # duration 1sec\n",
        "    def sensoryInput(self, parameter, simu, sigmaIn=None, paramf=None, f_stim=None, InMax=None, In0=None):\n",
        "        # paramf = np.arange(1, 101)\n",
        "        w = np.exp(-(((paramf or parameter.f) - (f_stim or self.f_stim)) ** 2) / (\n",
        "                2 * (sigmaIn or parameter.sigmaIn) ** 2))  # pars['f'] = 1:N\n",
        "\n",
        "        # totalAct = w.sum(axis = 1) #sum over each row\n",
        "        # norm_w = (w.T / totalAct).T # elementwise division\n",
        "        In = np.where(f_stim or self.f_stim > 0, (InMax or self.InMax) * w + (In0 or self.In0),\n",
        "                      0)  # if stim >0 give InMax * weight + In0 otherwise give 0\n",
        "        if self.tail != 0:\n",
        "            tail_zeros = np.zeros((parameter.N, int(self.tail / simu.dt)))\n",
        "            In = np.hstack((In, tail_zeros))\n",
        "\n",
        "        range_sim = np.arange(1, In.shape[0] + 1)\n",
        "        self.In = In\n",
        "        self.w = w\n",
        "        self.sigmaIn = sigmaIn\n",
        "\n",
        "        return In, range_sim, w, sigmaIn\n",
        "\n",
        "    def printStim(self):\n",
        "        print(\"frequence of stimulus f:\", self.f, \"  ISI:\", self.ISI, \" Size In:\", self.In.shape, \"Size w:\",\n",
        "              self.w.shape, \"  f_stim:\", self.f_stim.shape,\n",
        "              \"sigmaIn:\", self.sigmaIn)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_6Z0lXJFNiK"
      },
      "source": [
        "## Class Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "*\n",
        "*\n",
        "***********  CLASS MYMODEL\n",
        "*\n",
        "*\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MyModel_time(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel_time, self).__init__()\n",
        "\n",
        "        #--- Define other model parameters, layers, or components here if needed\n",
        "        self.dt = torch.tensor(1e-4) #sim.dt\n",
        "        self.N = 20\n",
        "        self.taue = self.taui = torch.tensor(0.005)\n",
        "         # ¤ parameter of the phi function Not tweakable parameters\n",
        "        self.ae = torch.tensor(18.26)  # 2 #Wong have to check # Modelling and Meg Gain of the E populaiton\n",
        "        self.be = torch.tensor(-5.38)  # Threshold of the E populaiton\n",
        "        self.hme = torch.tensor(78.67)\n",
        "        self.ai = torch.tensor(21.97)\n",
        "        self.bi = torch.tensor(-4.81)\n",
        "        self.hmi = torch.tensor(125.62)\n",
        "        #create the smallest possible number\n",
        "        self.epsilon = sys.float_info.epsilon\n",
        "\n",
        "        self.sIn = torch.tensor(.1)\n",
        "        self.sOut= 3.\n",
        "        self.sEI = .2\n",
        "        self.tauAMPA = torch.tensor(0.002)\n",
        "        self.tauGABA = torch.tensor(0.005)\n",
        "\n",
        "        self.wei = torch.tensor(dog_filter(self.sOut, int(self.N)), dtype=torch.float32)\n",
        "        self.wii = torch.tensor(np.eye(int(self.N)), dtype=torch.float32) # dog_filter(sIn, sOut, N)#np.eye(N) #\n",
        "        self.wie = torch.tensor(gaussian_filter(self.sEI, int(self.N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)\n",
        "        self.wes = torch.tensor(np.eye(int(self.N)), dtype=torch.float32)  # Identity matrix\n",
        "\n",
        "\n",
        "        # initial parameters\n",
        "        \"\"\"self.Jee = nn.Parameter(torch.tensor(0.072, requires_grad= True, dtype= torch.float64))#, requires_grad=False, dtype=torch.float32)#I replaced .072 by 0.072\n",
        "        #ic(self.Jee.grad_fn) #should be none\n",
        "        self.Jei = nn.Parameter(torch.tensor(0.004, requires_grad= True, dtype= torch.float64))\n",
        "        self.Jie = nn.Parameter(torch.tensor(0.05, requires_grad=True, dtype=torch.float64))\n",
        "        self.Jii = nn.Parameter(torch.tensor(0.6, requires_grad=True, dtype=torch.float64))\n",
        "        self.Jin = nn.Parameter(torch.tensor(0.00695, requires_grad= True, dtype=torch.float64))\"\"\"\n",
        "\n",
        "        # parameters after first training\n",
        "        \"\"\"self.Jee = nn.Parameter(torch.tensor(0.072, requires_grad= True, dtype= torch.float64))#, requires_grad=False, dtype=torch.float32)#I replaced .072 by 0.072\n",
        "        #ic(self.Jee.grad_fn) #should be none\n",
        "        self.Jei = nn.Parameter(torch.tensor(0.0042, requires_grad= True, dtype= torch.float64))\n",
        "        self.Jie = nn.Parameter(torch.tensor(0.05, requires_grad=True, dtype=torch.float64))\n",
        "        self.Jii = nn.Parameter(torch.tensor(0.6, requires_grad=True, dtype=torch.float64))\n",
        "        self.Jin = nn.Parameter(torch.tensor(0.0077, requires_grad= True, dtype=torch.float64))\n",
        "        \"\"\"\n",
        "\n",
        "        # parameters after second training\n",
        "        self.Jee = nn.Parameter(torch.tensor(0.072, requires_grad= True, dtype= torch.float64))#, requires_grad=False, dtype=torch.float32)#I replaced .072 by 0.072\n",
        "        #ic(self.Jee.grad_fn) #should be none\n",
        "        self.Jei = nn.Parameter(torch.tensor(0.0025, requires_grad= True, dtype= torch.float64))\n",
        "        self.Jie = nn.Parameter(torch.tensor(0.049, requires_grad=True, dtype=torch.float64))\n",
        "        self.Jii = nn.Parameter(torch.tensor(0.6, requires_grad=True, dtype=torch.float64))\n",
        "        self.Jin = nn.Parameter(torch.tensor(0.008, requires_grad= True, dtype=torch.float64))\n",
        "\n",
        "    def phi(self, I_tot, a, b, hm): #)))  # this use a lot of memory - exponential part\n",
        "        #multi= torch.nan_to_num((torch.mul(a, I_tot) + b), nan = self.epsilon, posinf=140, neginf=self.epsilon)\n",
        "\n",
        "        for i in range(I_tot.shape[1]):\n",
        "                if torch.isnan(I_tot[:,i])== True:\n",
        "                    ic(I_tot, i)\n",
        "                    exit()\n",
        "\n",
        "        mulan =torch.mul(a, I_tot)\n",
        "\n",
        "        multi= mulan + b\n",
        "\n",
        "        expo = torch.exp(- (multi))  #.abs()+ self.epsilon)\n",
        "        return torch.multiply(hm, torch.divide(1, (1+ expo)))\n",
        "\n",
        "    def forward(self, In):\n",
        "        #--- Initialize model variables here\n",
        "        prev_r_e = torch.zeros((In.shape[0], self.N)) # torch.ones(self.N) shows more obvious results\n",
        "        prev_r_i = torch.zeros((In.shape[0], self.N))\n",
        "        prev_s_ampa = torch.zeros((In.shape[0], self.N))\n",
        "        prev_s_gaba = torch.zeros((In.shape[0], self.N))\n",
        "        dr_e_dt = torch.zeros((In.shape[0], self.N))\n",
        "        dr_i_dt = torch.zeros((In.shape[0], self.N))\n",
        "        s_ampa = torch.tensor(0.)\n",
        "        i_tot_e = torch.tensor(0.)\n",
        "        i_tot_i = torch.tensor(0.)\n",
        "\n",
        "        for k in range(1, In.shape[0]):\n",
        "            #--- Compute values of interest\n",
        "            #the operation Jee_re = self.Jee * prev_r_e => triggers inplace error\n",
        "            s_gaba_wie = prev_s_gaba[k-1,:] @ self.wie\n",
        "            s_ampa_wei = prev_s_ampa[k-1,:] @ self.wei\n",
        "            s_gaba_wii = prev_s_gaba[k-1,:] @ self.wii\n",
        "            JeeAmpa =  torch.mul(self.Jee, s_ampa)\n",
        "            i_tot_e = torch.add(torch.subtract(JeeAmpa, torch.mul(self.Jie, s_gaba_wie)), torch.mul(self.Jin, In[k - 1, :]))\n",
        "            i_tot_i = torch.subtract(torch.mul(self.Jei, s_ampa_wei), torch.mul(self.Jii, s_gaba_wii))\n",
        "\n",
        "            phi_arr_e = self.phi(i_tot_e, self.ae, self.be, self.hme)\n",
        "            phi_arr_i = self.phi(i_tot_i, self.ai, self.bi, self.hmi)\n",
        "\n",
        "            dr_e_dt[k,:] = (-prev_r_e[k - 1, :] + phi_arr_e) / self.taue\n",
        "            dr_i_dt[k,:] = (-prev_r_i[k - 1, :] + phi_arr_i) / self.taui\n",
        "\n",
        "            r_e = prev_r_e[k - 1, :] + dr_e_dt[k,:] * self.dt\n",
        "            r_i = prev_r_i[k - 1, :] + dr_i_dt[k,:] * self.dt\n",
        "\n",
        "            dS_amp_dt = (- prev_s_ampa[k - 1, :] / self.tauAMPA) + r_e\n",
        "            s_ampa = prev_s_ampa[k - 1, :] + dS_amp_dt * self.dt\n",
        "\n",
        "            dS_gab_dt = (- prev_s_gaba[k - 1, :] / self.tauGABA) + r_i\n",
        "            s_gaba = prev_s_gaba[k - 1, :] + dS_gab_dt * self.dt\n",
        "\n",
        "            prev_r_e[k,:] = r_e\n",
        "            prev_r_i[k,:] = r_i\n",
        "            prev_s_ampa[k,:] = s_ampa\n",
        "            prev_s_gaba[k,:] = s_gaba\n",
        "\n",
        "\n",
        "            \"\"\"dr_e_dt = torch.div(torch.add(torch.neg(prev_r_e[k-1,:]), phi_arr_e), self.taue)\n",
        "            dr_i_dt = torch.div(torch.add(torch.neg(prev_r_i[k-1,:]), phi_arr_i), self.taui)\n",
        "            #ic(dr_e_dt.grad_fn)\n",
        "\n",
        "            r_e = torch.mul(torch.add(torch.neg(prev_r_e[k-1,:]), dr_e_dt), self.dt)# torch.multiply(), self.newfactor)\n",
        "            #ic(r_e.grad_fn, r_e.shape)\n",
        "            r_i = torch.mul(torch.add(torch.neg(prev_r_i[k-1,:]), dr_i_dt), self.dt)\n",
        "\n",
        "\n",
        "            dS_amp_dt = torch.add(torch.divide(- prev_s_ampa[k-1,:], self.tauAMPA), r_e)\n",
        "            s_ampa = torch.mul(torch.add(prev_s_ampa[k-1,:], dS_amp_dt), self.dt)\n",
        "            #ic(dS_amp_dt.grad_fn, s_ampa.grad_fn)\n",
        "            dS_gab_dt = torch.add(torch.divide(- prev_s_gaba[k-1,:], self.tauGABA), r_i)\n",
        "            s_gaba = torch.mul(torch.add(prev_s_gaba[k-1,:], dS_gab_dt), self.dt)\"\"\"\n",
        "\n",
        "\n",
        "        return prev_r_e, prev_r_i, dr_e_dt, dr_i_dt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R88oSVXFTVb"
      },
      "source": [
        "# Create stimuli Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Xs-oU-FY2E"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "*\n",
        "*  Creat IN and Forward pass\n",
        "*\n",
        "*\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    ## Parameters used to create In\n",
        "N = 20\n",
        "# \\\\\\\\\\\\\\\\\\\\\\ Parameters\n",
        "param = Parameter(N=20, sIn=.1, sOut=3., sEI=.2)\n",
        "# \\\\\\\\\\\\\\\\\\\\\\ Simulation time\n",
        "simu = Simulation(1e-4, .4)  # dt #rangeSim #dur = 2s\n",
        "\n",
        "# \\\\\\\\\\\\\\\\\\\\\\ Bottom up sensory input\n",
        "stimuli = Stim(param, simu, dur=.3, f=[8], ISI=.05)  # dur = 1s Isi=1s\n",
        "In, range_sim, w, sigmaIn = stimuli.sensoryInput(param, simu, sigmaIn=2.)\n",
        "\n",
        "J1 = {'Jee': 0.072, 'Jei': 0.004, 'Jie': 0.05, 'Jii': 0.6, 'Jin': 0.00695}\n",
        "J_list = list(J1.keys())\n",
        "\n",
        "# +++++++++++++++++++++++++ Initialize the Model ++++++++++++++++++++++++++++\n",
        "mymodel = MyModel_time()\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN forward pass and Print heatmap ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "sti = torch.tensor(stimuli.In, dtype=torch.float32)\n",
        "r_e, r_i, dredt, dridt = mymodel.forward(sti)\n",
        "print(torch.max(r_e[1000,:]))\n",
        "HeatMap(r_e.detach().numpy(), r_i.detach().numpy(), J1)\n",
        "HeatMap(sti.detach().numpy(), r_i.detach().numpy(), J1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "OXFvdt6EGNZV",
        "outputId": "3d8f5b6a-d401-4a72-8ec5-af721169b71b"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(0, r_e.shape[0]), make_it_proba(r_e)[:,7].detach().numpy(), label = \"Excitatory neurons\")\n",
        "plt.plot(np.arange(0, r_i.shape[0]), make_it_proba(r_i)[:,7].detach().numpy(), label = \"Inhibitory neurons\")\n",
        "plt.title(\"Neurons activity at position 11 for a stimuli of 9\")\n",
        "plt.xlabel(\"time ms\")\n",
        "plt.ylabel(\"Proba of Firing rate\")\n",
        "plt.legend()\n",
        "\"\"\"\n",
        "plt.plot(np.arange(0, r_e.shape[1]), make_it_proba(r_e)[800,:].detach().numpy(), label = \"Excitatory neurons\")\n",
        "plt.plot(np.arange(0, r_i.shape[1]), make_it_proba(r_i)[800,:].detach().numpy(), label = \"Inhibitory neurons\")\n",
        "plt.title(\"Neurons activity at position 11 for a stimuli of 9\")\n",
        "plt.xlabel(\"time ms\")\n",
        "plt.ylabel(\"Proba of Firing rate\")\n",
        "plt.legend()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlZSpYE2FcRZ"
      },
      "source": [
        "# Optimization part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Space for Loss design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_loss_(Pred, Target, derivativeE, derivativeI):\n",
        "  stimu_pop= torch.argmax(Target).item()\n",
        "  print(stimu_pop)\n",
        "  tensor_for_cost_derivative_E = derivativeE.clone() # torch.zeros((r_e.shape))\n",
        "  tensor_for_cost_derivative_I = derivativeI.clone() # torch.zeros((r_e.shape))\n",
        "\n",
        "  loss_proba = torch.zeros((Pred.shape))\n",
        "  Cost = torch.tensor(0.0)\n",
        "  for t in range(Pred.shape[0]):\n",
        "    #1/ Proba term\n",
        "    #m = nn.LogSoftmax()#dim=1\n",
        "    #m = make_it_proba()\n",
        "    #Ypred = m(r[t,:])\n",
        "    target_pop_long = Target.select(0,t).long() #[t,:], dtype = torch.long)\n",
        "    pred_pop = Pred.select(0,t) #[t,:]#.long()\n",
        "  # Calculate loss by comparing the distribution to the expected probabilities\n",
        "    loss_proba[t,:] = F.nll_loss(pred_pop, target_pop_long)#, reduction='none')  #negative log likelihood # why reduction = none???\n",
        "\n",
        "  #sum over time step\n",
        "  total_loss_proba = torch.sum(loss_proba)\n",
        "  ic(total_loss_proba.grad_fn)\n",
        "  #2/ derivative good\n",
        "  # for excitatory neurons\n",
        "  tensor_for_cost_derivative_E[:,stimu_pop] = torch.neg(derivativeE.select(1, stimu_pop)) #we want it to be negative for the stimulated population\n",
        "  \n",
        "  #for inhibitory neurons\n",
        "  tensor_for_cost_derivative_I = torch.neg(derivativeI)\n",
        "  tensor_for_cost_derivative_I[:, stimu_pop] = torch.neg(derivativeI.select(1, stimu_pop)) #[:,stimu_pop])\n",
        "  #tensor_for_cost_derivativeI = [i for i in derivativeI if i not in target_arr]\n",
        "\n",
        "  loss_derivative = torch.add(torch.sum(torch.mean(F.softplus(tensor_for_cost_derivative_E)**2, axis = 0)),\n",
        "                              torch.sum(torch.mean(F.softplus(tensor_for_cost_derivative_I)**2, axis = 0)))\n",
        "  \n",
        "  Cost = torch.add(loss_derivative, total_loss_proba) #loss_proba + loss_derivative\n",
        "\n",
        "  return Cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_loss_(r_e, Y_target, dredt, dridt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ic(grid_x[range(0,20), 3].shape)\n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_neg_log_likelihood(Y_prediction, Y_target):\n",
        "    #https://stackoverflow.com/questions/49602205/python-numpy-negative-log-likelihood-calculation-when-some-predicted-probabiliti\n",
        "    return -special.xlogy(Y_target , Y_prediction) - special.xlogy(1-Y_target, 1-Y_prediction ) #-Y_prediction[range(0, 20), Y_target].mean() # F.nll_loss(Y_prediction, Y_target) #(((Y_prediction - Y_target)**2))\n",
        "\n",
        "grid_x, grid_y = np.mgrid[0:1:20j, 0:1:20j]\n",
        "print(grid_x)\n",
        "values = custom_neg_log_likelihood(grid_x, grid_y)\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, num=0, figsize=(16, 8),\n",
        "                       subplot_kw={'projection': '3d'})\n",
        "pSurf = ax.plot_surface(grid_x, grid_y, values, rstride=1, cstride=1, cmap='rainbow')\n",
        "#ax = fig.add_subplot(111, projection='3d')\n",
        "#ax.plot_surface(x, y, z)\n",
        "fig.colorbar(pSurf)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Discrete Loss')\n",
        "#I have a vector y of real labels. I have a vector p of estimated probabilities.\n",
        "#ax.set_title(\"Loss function: squared difference\")\n",
        "ax.set_title(\"Loss function: neg log likelihood\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def gradientDescent\n",
        "def customLoss(Prediction, Target):\n",
        "    \n",
        "    return customLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization Loop function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def easyLoss(Y_pred_prob,target):\n",
        "    #loss = torch.sum(torch.sum((Y_prediction_prob-Y_target), axis =1))\n",
        "    return torch.mean((Y_pred_prob - target)**2)\n",
        "\n",
        "def optimizerloop(model, input, target, loss_f, Opt_name,learningRate, num_epoch):\n",
        "    losses = torch.zeros(num_epoch) # used to plot the loss at the end\n",
        "    \n",
        "    if Opt_name == \"SGD\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=learningRate, weight_decay = 0.0001)#, weight_decay = 0.00001) #, weight_decay = 0.00001)#, weight_decay = 0.001)#0.989\n",
        "    elif Opt_name == \"Adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                      lr=learningRate)#, weight_decay = 0.00001) #, weight_decay = 0.00001)#, weight_decay = 0.001)#0.989\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epoch):\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate output\n",
        "        Y_prediction, I, dredt, dridt = model(input)\n",
        "  \n",
        "        # calculate loss\n",
        "        Y_prediction_prob = make_it_proba(Y_prediction)\n",
        "        #loss = torch.sum(torch.sum((Y_prediction_prob-Y_target), axis =1))# (torch.mean((Y_prediction_prob - Y_target)**2))\n",
        "        #loss = loss_f(Y_prediction_prob, target) #t\n",
        "        loss = torch.mean(custom_loss_(Y_prediction_prob, target, dredt, dridt))\n",
        "        ic(loss.grad_fn)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        losses[epoch] = loss\n",
        "        \"\"\"\n",
        "        print(f'Epoch [{epoch + 1}/{num_epoch}], Loss: {loss}, Loss_grad: {loss.grad_fn}')  # .item()\n",
        "        for i, par in enumerate(model.parameters()):\n",
        "           ic(J_list[i], par, par.grad)\n",
        "           \"\"\"\n",
        "\n",
        "    return list(model.parameters()), losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# +++++++++++++++++++++++++ Inputs + Labels +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "X_input = sti\n",
        "Y_target = get_expected_Y_relu(X_input) #get the expected dirac delta for our particular Input\n",
        "#SGD + Mean Loss\n",
        "num_epochs = 5\n",
        "lr = .00001\n",
        "param_new, losses_list = optimizerloop(mymodel, X_input, Y_target,easyLoss,\"SGD\",lr, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(losses_list.shape[0]), losses_list.detach().numpy(), 'bo', label='Training loss')\n",
        "plt.title(f\"Loss over {num_epochs} epochs for a learning rate of {lr}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVeeTCr5c432"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "*\n",
        "*  OPTIMIZATION PART\n",
        "*\n",
        "*\n",
        "\"\"\"\n",
        "\n",
        "# +++++++++++++++++++++++++ Optimizer ++++++++++++++++++++++++++++\n",
        "learning_rate = 0.00001 #0.001\n",
        "optimizer = optim.SGD(mymodel.parameters(),\n",
        "                      lr=learning_rate, weight_decay = 0.0001)#, weight_decay = 0.00001) #, weight_decay = 0.00001)#, weight_decay = 0.001)#0.989\n",
        "\n",
        "\n",
        "\n",
        "# +++++++++++++++++++++++++ Epochs +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "num_epochs = 5\n",
        "\n",
        "# +++++++++++++++++++++++++ Inputs + Labels +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "X_input = sti\n",
        "Y_target = get_expected_Y_relu(X_input) #get the expected dirac delta for our particular Input\n",
        "losses = torch.zeros(num_epochs) # used to plot the loss at the end\n",
        "\n",
        "# +++++++++++++++++++++++++ Problems investigations +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# import tracemalloc\n",
        "# tracemalloc.start()\n",
        "mymodel.train()\n",
        "criterion = nn.CrossEntropyLoss()#.cuda()\n",
        "\n",
        "# +++++++++++++++++++++++++ Optimization loop +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "with torch.autograd.set_detect_anomaly(False):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Create a new input tensor for each epoch\n",
        "        #X_input.requires_grad = False\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate output\n",
        "        Y_prediction, I, dredt, dridt = mymodel(X_input)\n",
        "        #ic(Y_prediction.grad_fn)\n",
        "\n",
        "        # calculate loss\n",
        "        Y_prediction_prob = make_it_proba(Y_prediction)\n",
        "        #loss = torch.sum(torch.sum((Y_prediction_prob-Y_target), axis =1))# (torch.mean((Y_prediction_prob - Y_target)**2))\n",
        "        loss = criterion(Y_prediction_prob, Y_target) #t\n",
        "\n",
        "        #ic(loss.grad_fn)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm_(mymodel.parameters(), -5, 5)  # Adjust max_norm as needed\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        losses[epoch] = loss\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss}, Loss_grad: {loss.grad_fn}')  # .item()\n",
        "        for i, par in enumerate(mymodel.parameters()):\n",
        "           ic(J_list[i], par, par.grad)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# check if input has zeros\n",
        "#torch.all(losses) # return True if there are zeros, otherwise return False\n",
        "\n",
        "# check if input has nans\n",
        "#torch.any(torch.isnan(losses)) # return True if there are nans, otherwise return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dmzDl4FFFDf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "*\n",
        "*  Plot the loss over epochs\n",
        "*\n",
        "\"\"\"\n",
        "\n",
        "plt.plot(np.arange(losses_list.shape[0]), losses_list.detach().numpy(), 'bo', label='Training loss')\n",
        "\n",
        "plt.title(f\"Loss over {num_epochs} epochs for a learning rate of {learning_rate}\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
