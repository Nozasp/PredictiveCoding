import pandas as pd
import numpy as np
import math
import random
import sys 
import torch
import torch.nn as nn
import torch.nn.functional as F  # F.mse_loss F.softmax
import torch.optim as optim #optim.sgd
from torchvision import transforms, utils, datasets
from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler
from torch.utils.data import TensorDataset, DataLoader  # for batch and split Xtrain Ytrain dataset
import scipy
import scipy.ndimage as nd
from scipy.stats import norm
from scipy.optimize import minimize
from scipy.interpolate import griddata
from scipy import special
from scipy.stats import truncnorm
import scipy.stats as stats
import numba
from numba import jit, cuda
# to measure exec time
from timeit import default_timer as timer   

from locale import format
from dataclasses import dataclass, MISSING
from sklearn import preprocessing #preprocessing.normalize


import seaborn as sns
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.gridspec as gridspec


from icecream import ic  # for debugging. print variable name
import pickle # for saving variable
import json # for saving variable in text file 


from PC_param import default_parameters_network # To get the parameters
pars = default_parameters_network()


""" Plot HeatMap of firing rate function"""
def HeatMap(rE, rI, J=None, toshow = True):
    if J == None:
        J = [.00989, 0.0081, .1, .87, .00081]  # J = dict(Jin=.008, Jee= .2, Jie=.2, Jei=1.4, Jii=6.7)
    if type(J) == dict:
        J = np.round(np.array(list(J.values())), 4)
        

    rE_df = pd.DataFrame(rE.T)  # to get time vs pop
    rI_df = pd.DataFrame(rI.T)
    rE_df.index = rE_df.index + 1
    rI_df.index = rI_df.index + 1
    rE_df.index.name, rI_df.index.name = ["Excitatory Population", "Inhibitory Population"]
    rE_df.columns.name, rI_df.columns.name = ["Time ms", "Time ms"]
    # print(rE_df.loc[[10]])

    # set context for the upcoming plot
    sns.set_context("notebook", font_scale=.8, rc={"lines.linewidth": 2.5, 'font.family': 'Helvetica'})

    fig, (axA, axB) = plt.subplots(2, 1, figsize=(6, 6))

    sns.heatmap(rE_df, ax=axA, cmap="viridis")
    sns.heatmap(rI_df, ax=axB)
    ##J2= {'Jee':  'Jei': , 'Jie': mpy(), 'Jii': mo), 'Jin': umpy()}
    axA.set_title(f"Firing rate in Hz of exc populations over time. Jie: {J[2]}, Jee: {J[0]}, Jin: {J[4]}",
                  fontdict={"fontsize": 10})
    axB.set_title(f"Firing rate in Hz of inh populations over time. Jei: {J[1]}, Jii: {J[3]}",
                  fontdict={"fontsize": 10})
    plt.tight_layout()

    if toshow:
        plt.show()

    else: 
        return fig, (axA, axB)
    #num_fig = random.randint(0, 1000)
    #fig.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/OutputNetwork/Heatmap/Heatmap_{num_fig:0>3}.png')

def connectivity_matrix_toBlend(model_T, Learning_Rate, n_epoch, sparce = "_", distance_diag = "_", rand = np.random.randint(0,999)):
    cmap1 = 'viridis'
    #cmap = 'plasma'
    cmap2 = 'magma'
    W_t = {}
    for name, par in model_T.named_parameters() : # enumerate(trained_model.parameters()):
        W_t[name] = par #getattr(trained_model, "wee)
        if name.startswith("J"):
            W_t.popitem()
            break

    W_list = list(W_t.items())
    wei_name = W_list[0][0]
    wei_value = W_list[0][1]
    wie_name = W_list[1][0]
    wie_value = W_list[1][1]
    num_fig_st = str(rand)

    fig_wei = plt.figure()
    axes_wei = fig_wei.add_subplot(111)    
    # using the matshow() function 
    caxes_wei = axes_wei.matshow(wei_value.detach().numpy(), cmap = cmap1)#interpolation ='nearest')
    fig_wei.colorbar(caxes_wei)
    fig_wei.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/ConnectivityMatrix/connectivity_matrix_{Learning_Rate}_{n_epoch}_AfterTraining_{sparce}_{distance_diag}_{wei_name}_{num_fig_st:0>3}.png')
    fig_wei.tight_layout()
    fig_wei.show()
    
    
    fig_wie = plt.figure()
    axes_wie = fig_wie.add_subplot(111)
    caxes_wie = axes_wie.matshow(wie_value.detach().numpy(), cmap = cmap2)#interpolation ='nearest')
    fig_wie.colorbar(caxes_wie)
    # Adds subplot 'ax' in grid 'gs' at position [x,y]
    fig_wie.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/ConnectivityMatrix/connectivity_matrix_{Learning_Rate}_{n_epoch}_AfterTraining_{sparce}_{distance_diag}_{wie_name}_{num_fig_st:0>3}.png')
    fig_wie.tight_layout()
    fig_wie.show()
    
    #plt.show()

def connectivity_matrix_2x2(model_T, Learning_Rate, n_epoch, sparce = "_", distance_diag = "_", rand = np.random.randint(0,999)):
    cmap1 = 'viridis'
    #cmap = 'plasma'
    cmap2 = 'magma'
    W_t = {}
    for name, par in model_T.named_parameters() : # enumerate(trained_model.parameters()):
        W_t[name] = par #getattr(trained_model, "wee)
        if name.startswith("J"):
            W_t.popitem()
            break

    W_list = list(W_t.items())
    wei_name = W_list[0][0]
    wei_value = W_list[0][1]
    wie_name = W_list[1][0]
    wie_value = W_list[1][1]
    num_fig_st = str(rand)# np.random.randint(0,999))

    fig_m = plt.figure()
    gs = gridspec.GridSpec(1, 2)

    ax_wei = plt.subplot(gs[0,0])
    ax0_wei = ax_wei.matshow(wei_value.detach().numpy() , cmap = cmap1)
                # Adds subplot 'ax' in grid 'gs' at position [x,y]
    ax_wei.set_ylabel(wei_name)
    fig_m.colorbar(ax0_wei,fraction=0.046, pad=0.04, orientation='horizontal').ax.tick_params(labelsize=10)
    fig_m.add_subplot(ax_wei)
    
    ax_wie = plt.subplot(gs[0,1])
    ax0_wie = ax_wie.matshow(wie_value.detach().numpy() , cmap = cmap2)
                # Adds subplot 'ax' in grid 'gs' at position [x,y]
    ax_wie.set_ylabel(wie_name)
    fig_m.colorbar(ax0_wie,fraction=0.046, pad=0.04, orientation='horizontal').ax.tick_params(labelsize=10)
    fig_m.add_subplot(ax_wie)
    
    #fig_m.colorbar(caxes_wei)
    fig_m.suptitle(f'Connectivity matrix obtained with LR = {Learning_Rate}, Epochs = {n_epoch} \n , discouraging term ={distance_diag}',  y= 0.83)#, fontsize=16
    fig_m.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/ConnectivityMatrix/connectivity_matrix_{Learning_Rate}_{n_epoch}_AfterTraining_{sparce}_{distance_diag}_{num_fig_st:0>3}.png')
    fig_m.tight_layout()
    fig_m.show()
    
    
   
    
    #plt.show()

# Make a dictionary a list if I want to access index! dictionary has no indexing (storage with keys)

def plot_connectivity_matrix(model_T, num_fig, Learning_Rate, n_epoch, limit = 2, sparce = "_", distance_diag = "_"):
    labelsize = 10
    subtitle_size = 11
    #cmap = 'viridis'
    #cmap = 'plasma'
    cmap = 'magma'

    W_t = {}
    for name, par in model_T.named_parameters() : # enumerate(trained_model.parameters()):
        W_t[name] = par #getattr(trained_model, "wee)
        if name.startswith("J"):
            W_t.popitem()
            break
        
    #print(len(W_t))

    column = 0
    row = 1
    for i in range(len(W_t)):
        column += 1
        if column == limit +1:
            row +=1
            column = 0 

    if len(W_t) >=limit:
        column = limit
    else:
        column = len(W_t)
    
    ic(row, column)
    fig_m = plt.figure()
    # create figure window

    gs = gridspec.GridSpec(row, column)
    # Creates grid 'gs' of a rows and b columns

    """for r in range(row):
            for c in range(column):
                print(r, c)
    """
    ic(W_t.keys)
    W_list = list(W_t.items())
    
    if row > 0:
        index = 0
        for r in range(row):
            for c in range(column):
                name = W_list[index][0]
                w = W_list[index][1]
                ic(r, c, name)
                ax = plt.subplot(gs[r,c])
                ax0 = ax.matshow(w.detach().numpy() , cmap = cmap)
                # Adds subplot 'ax' in grid 'gs' at position [x,y]
                ax.set_ylabel(name)
                #ax.set_title(f"{name} trained", size = subtitle_size)
                #ax.figure.axes[index].tick_params(axis="both", labelsize= labelsize) 
                
                fig_m.colorbar(ax0,fraction=0.046, pad=0.04, orientation='horizontal').ax.tick_params(labelsize=10)
                fig_m.add_subplot(ax)

                index +=1
                if index == len(W_t):
                    break
                else:
                    continue
                
    num_fig_st = str(num_fig)
    if row == 1:
        fig_m.suptitle(f'Connectivity matrix obtained with LR = {Learning_Rate}, Epochs = {n_epoch}',  y= 0.83)#, fontsize=16
    else: 
        fig_m.suptitle(f'Connectivity matrix obtained with LR = {Learning_Rate}, Epochs = {n_epoch}')#, fontsize=16
    fig_m.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/ConnectivityMatrix/connectivity_matrix_{Learning_Rate}_{n_epoch}_AfterTraining_{sparce}_{distance_diag}_{num_fig_st:0>3}.png')
    fig_m.tight_layout()
    fig_m.show()
    #plt.show()




""" Filters gauss and Dog and LoG"""
def gaussian_filter(s, N):
    pop = np.arange(1, N + 1)
    n = 1 / (np.sqrt(2 * np.pi) * N * s)
    gaussW = n * np.exp(-(pop - pop[:, np.newaxis]) ** 2 / (2 * s ** 2))
    gaussW2 = gaussW / (.009 ** 2 / np.max(gaussW))  # 1
    return gaussW2

def dog_filter(sOut, N):
    sIn = sOut / 30
    pop = np.arange(1, N + 1)
    gaussIn = np.exp(-(pop - pop[:, np.newaxis]) ** 2 / (2 * sIn ** 2))
    gaussOut = np.exp(-(pop - pop[:, np.newaxis]) ** 2 / (2 * sOut ** 2))
    dog = gaussOut - gaussIn
    if np.max(dog) == 0 or None:
        print('zero max')
        dog = 0
    else:
        dog = dog / (.042 ** 2 / np.max(dog))  # .0088
    return dog

def LoG_filter(s, N):
    x_lap = np.eye(N)
    lapl_filter = nd.gaussian_laplace(x_lap, sigma=(s, s))
    return lapl_filter

def dLogGaus(s=.61, N=20):
    dig = LoG_filter(s, N) + gaussian_filter(.019 * s, N)
    return dig



def truncated_normal(N, mu=0., sigma= .1):
    sigma = 1/np.sqrt(N)
    lower, upper = 0., np.inf
    pop = np.arange(1, N + 1)
    
    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)
    #x = np.linspace(0, size, size)

    return X.pdf(pop- pop[:, np.newaxis])#X.rvs(N)

def init_random_matrix(N, mu = 0.):

    lower, upper = 0., np.inf
    sigma = 1/np.sqrt(N)#random.randint(1, 30), random.uniform(0.1, 5)
    X = stats.truncnorm(
        (lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)
    rnd_matrix = np.zeros((N, N))
    for i in range(N):
        for j in range(N):
            rnd_matrix[i][j] = X.rvs(1)# np.random.uniform(low=0.0, high=1.0, size=1) #X.rvs(1)
    return rnd_matrix 

def init_random_Tgaussian(N, mu =0., sigma = .025, lower = 0., upper = np.inf): #this simga! standard deviation not sigma2
    #lower, upper = 0., np.inf
    #sigma = 1/np.sqrt(N)                                                                  #random.randint(1, 30), random.uniform(0.1, 5)
    X = stats.truncnorm(
        (lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)

    rnd_tensor = X.rvs(N) 

    return torch.tensor(rnd_tensor)

"""#
    rnd_tensor = np.zeros(N)
    for i in range(N):"""


#print(P.shape)
"""
 Normalization or transform to probability
"""
"""def make_it_proba(r_e):
    eps = torch.FloatTensor([sys.float_info.epsilon]) #torch.FloatTensor
    #noisy_term = init_random_Tgaussian(r_e.shape[1])
    #plt.plot(np.arange(r_e.shape[1]), r_e[40,:].detach().numpy())
    #plt.show()
    offset = 1# 5
    #sigma = 1/np.sqrt(r_e.shape[1])= 0.025
    sigma = 0.1 #0.025
    noisy_term = offset + torch.tensor(init_random_Tgaussian(r_e.shape[1], mu = 0., sigma = sigma))
    #noisy_term = torch.max(offset, torch.tensor(init_random_Tgaussian(r_e.shape[1], mu = 0., sigma = sigma), dtype = torch.float32))
    plt.plot(np.arange(r_e.shape[1]), noisy_term)
    plt.show()
    
    #r_e_max = torch.max(r_e, eps.expand_as(r_e))#
    #threshold = 5     # torch.FloatTensor([5]).expand_as(r_e[10,:])
    #sum_r_e = torch.sum(r_e, 1).reshape(r_e.shape[0], 1)
    #prob_r= 0.5 * (1 + torch.tanh(sum_r_e - threshold))*(r_e_max / sum_r_e)   #r_e_max.sum())

    unormalised_prob = r_e + (noisy_term * eps) 
    prob_r = unormalised_prob / sum(unormalised_prob, 1) #.reshape(r_e.shape[0], 1) #keepdim = True
    #plt.plot(np.arange(r_e.shape[1]), prob_r[40,:].detach().numpy())
    #plt.show()
    return prob_r #.reshape(r_e.shape[0], r_e.shape[1]) 
"""

def make_it_proba(r_e, offset = 1):
    eps = torch.FloatTensor([sys.float_info.epsilon]) #torch.FloatTensor
    #offset = 0.5 #eps #1 # 5
    sigma = 0.025#works well 0.025 #works0.1 #0.01 #0.025
    noisy_term = torch.tensor(init_random_Tgaussian(r_e.shape[1], mu = 1, sigma = sigma, lower = 1))#offset +  #, dtype = torch.float32
    unormalised_prob = r_e + (noisy_term)
    prob_r = unormalised_prob / (unormalised_prob).sum(1, keepdims = True)
    return prob_r 


def softmax(x):
    return torch.exp(x) / torch.sum(torch.exp(x), axis=1, keepdims=True)

''' Classic Normalization
use:
#preprocessing.normalize(re_numpy, axis= 0)[20,:].sum()
or use:
'''
def normalize(x):
    N = (x - x.min()) / (x.max() - x.min())
    return N


"""
Differentiable function for back propagation

To avoid non-differentiable araising from discontinuity of the function, I "relax" (smoothen) the where() expression by using a sigmoid instead
*   with grad_fn:
*   if I get : > <SumBackward1 object at 0x7f79da0b9520> # differentiable
*   else I get none
"""
def relu_stim(x, stim):
    return torch.nn.functional.relu(1.0 - torch.abs(x - stim),
                                    inplace=False)  # inplace = False to avoid implace operation

def Dirac(A, N=pars["NumN"]):
    y = scipy.signal.unit_impulse(N, idx=(torch.max(torch.argmax(A))))  
    return torch.tensor(y)

def replace_argmax(r):
    # along some dimension (e.g., the last dimension).
    indices = torch.arange(r.shape[-1]).to(r.device)
    return torch.gather(indices, dim=-1, index=torch.argmax(r, dim=-1)).max()



""" Target design: Get the expected stimuli and then create a matrix of 1 where stimuli 0 elsewhere"""

def get_stimuli_input(X_train_tensor):  # input of the shape Xtrain_tensor[5,:,:]
    Xargmax = torch.argmax(X_train_tensor, dim=1) #consider replacing argmax by replace_argmax
    Xmax = torch.max(Xargmax)
    return Xmax

def get_expected_Y_relu(X_train_tensor):
    x_t = torch.transpose(X_train_tensor, 0, 1)
    dirac_2d = torch.zeros(x_t.shape)
    stim = get_stimuli_input(
        X_train_tensor)  # input of the shape Xtrain_tensor[5,:,:] # here get_stimuli not differenciable
    
    for pop, t in enumerate(x_t):
        tpop = torch.tensor(pop)# replace where function by relu functio which is differentiable
        dirac_2d[pop, :] = torch.nn.functional.relu(1.0 - torch.abs(tpop - stim), inplace=False).requires_grad_(False)
    dirac_2d = torch.transpose(dirac_2d, 1, 0)
    return dirac_2d



""" 
Optimization function
Make a function which save parameters of trained model and upload the new model with the updated parameters
"""
def model_with_saved_trained_param(old_model, optimizer, Model, param, sim, dicJ):
    # or to save the parameters only
    torch.save(old_model.state_dict(),"Old_model_optimized_parameters.pth")
    torch.save(optimizer.state_dict(),"optimizer_optimized_parameters.pth")
    #load these parameters in a new model instance
    new_mymodel = Model(param, sim, dicJ)
    new_mymodel.load_state_dict(torch.load("Old_model_optimized_parameters.pth")) 
    optimizer.load_state_dict(torch.load('optimizer_optimized_parameters.pth'))

    #print(optimizer.param_groups[0]['params'])
    if old_model.Jee == new_mymodel.Jee:
        print("it works")
    print("old model Jee:",old_model.Jee,"new model Jee:", new_mymodel.Jee)
    #print(optimizer.param_groups)
    #print(optimizer.state)
    return new_mymodel, optimizer


def load_weights(newmodel, modelpath): #string
        if '.pt' not in modelpath:
            modelpath += '.pt'      
        newmodel.load_state_dict(torch.load(modelpath))
        #new_mymodel = Model(param, sim, model.state_dict())
        return newmodel #, newmodel.state_dict() to access the param
    
def save_weights(oldmodel, modelpath, epoch=None):  #string
    if '.pt' not in modelpath:
            modelpath += '.pt'
    torch.save(oldmodel.state_dict(), modelpath)  


"""
Model evaluation
function wich test the accuracy of a model with new parameters compared to expected results + loss values for every samples
"""
def test_model(model, test_dataloader, loss_f):
    model.eval()  # Set the model to evaluation mode
    test_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for x_test, y_test in test_dataloader:
            # Calculate output
            Y_prediction, _, dredt, dridt = model(x_test[0])
            #Y_prediction_prob = make_it_proba(Y_prediction)

            # Calculate loss
            #loss = loss_f(Y_prediction_prob, y_test[0], dredt, dridt)
            loss = loss_f(Y_prediction, y_test[0], dredt, dridt)

            # Accuracy
            predictions = Y_prediction.detach().round() # rounds the predictions to the nearest integer (0 or 1), assuming they represent probabilities.
            #predictions = Y_prediction_prob.detach().round() # rounds the predictions to the nearest integer (0 or 1), assuming they represent probabilities.
            correct_predictions += (predictions == y_test[0]).sum().item() # calculates the number of correct predictions by comparing the rounded predictions with the true labels (y_test). It sums up the correct predictions over the batch.
            total_samples += y_test[0].numel() # adds the total number of samples/item in the current batch to the overall count.

            test_loss += loss.item()

    accuracy = correct_predictions / total_samples
    average_loss = test_loss / len(test_dataloader)

    print(f'Test Accuracy: {accuracy:.4f}, Test Loss: {average_loss:.4f}')
    return accuracy, average_loss


""" 
LOSS
"""
def easyLoss(Y_pred_prob,target):
    #loss = torch.sum(torch.sum((Y_prediction_prob-Y_target), axis =1))
    return torch.mean((Y_pred_prob - target)**2)

#for l2 reg with w params
def sum_relu(w):
      return torch.sum(F.relu(-w))


""" 
Run simulation over batchXtime
"""
def run_model_across_batch(Input, len_sim, model_instance):
    count = 0 
    P0 = torch.zeros_like(Input)  
    I0 = torch.zeros_like(Input)
    dPdt = torch.zeros_like(Input)  
    model_instance.initiate_state()

    for i in range(Input.shape[0]):
            count +=1
            P0[i,:], I0[i,:], dPdt[i,:], dridt, ampa, gaba = model_instance.forward(Input[int(i),:])#.item()
            if count == len_sim: #train_IN.shape[1]: #if we end the simulation time and go to the next batch
                count = 0
                model_instance.initiate_state()

    return P0, I0, dPdt

""" 
Plot Normalised predictions
"""
def plot_normalized_plot(P0, Ptrained, t, legend = None): #legend = list of 2 strings 
    P0_np = np.array(P0.detach().numpy())
    PT_np = np.array(Ptrained.detach().numpy())

    P0_norm = preprocessing.normalize(P0_np, axis= 1) 
    PT_norm = preprocessing.normalize(PT_np, axis= 1) 
    if legend is None:
        plt.plot(np.arange(P0_norm.shape[1]), P0_norm[t,:], label = f"{t}ms, untrained")
        plt.plot(np.arange(PT_norm.shape[1]), PT_norm[t,:], label = f"{t}ms, trained")
    else:
        plt.plot(np.arange(P0_norm.shape[1]), P0_norm[t,:], label = f"{t}ms," + legend[0])
        plt.plot(np.arange(PT_norm.shape[1]), PT_norm[t,:], label = f"{t}ms," + legend[1])

""""
Compute level of sparcity
"""
def measure_sparcity(A):
    return 1.0 - ( np.count_nonzero(A) / float(A.size) )

""""
Make a logistic matrix for penalizing long term connections in inh pop
"""
def logistic_func_dd(N, k= 1):#2#.35 # goo1): #.35): #.35): or 1 and 8
  L = 1# 8*N #5*N
  x_diff = np.abs(np.arange(N)[np.newaxis, :] - np.arange(N)[:, np.newaxis])
  return L/(1+ np.exp(-k*(x_diff-4)))#4 good good !#2#3good#4 good #5 good #6 #8))) #(12))))#N/3


 
# ***************** CLASS ***************************************

@dataclass
class Parameter:
    # °°° Load the parameters °°°
    taue: float = pars["taue"]
    ae: float = pars['ae']
    be, hme, I_noise = pars['be'], pars['hme'], pars['I_noise']
    Jee: float = pars['Jee']
    taui, ai, bi, hmi = pars['taui'], pars['ai'], pars['bi'], pars['hmi']
    tauNMDA, tauAMPA, tauGABA = pars['tauNMDA'], pars['tauAMPA'], pars['tauGABA']
    gamma: float = pars['gamma']  # nmda coupling parameter from brunel
    c_dash = pars['c_dash']
    sigma = pars['sigma']  # param.sigma = .0007 for Noise
    I_noise = pars['sigma'] * np.random.randn(3, 1)
    I1 = pars['Jext'] * pars['mu0'] * (1 + pars['c_dash'] / 100)
    I2 = pars['Jext'] * pars['mu0'] * (1 - pars['c_dash'] / 100)
    # I1, I2 = pars['I1'], pars['I2']
    sigmaIn = pars['sigmaIn']

    # Input parameters
    In0 = pars['In0']  # % Spontaneous firing rate of input populations (Hz)
    InMax = pars['InMax']  # % Max firing rate of input populations (Hz)
    Iq0 = pars['Iq0']  # % Spontaneous firing rate of feedback populations (Hz)
    IqMax = pars['IqMax']  # % Max firing rate of feedback populations (Hz)

    # Gaussian filter
    # sIn = pars['sigmaInh'][0]
    # sOut = pars['sigmaInh'][1]

    #°°° Hard encode these parameters °°°
    Jee: float = pars['Jee']#0.2609
    Jii: float = pars['Jii']
    Jei: float = pars['Jei']
    Jie: float = pars['Jie']
    #Jes, Jsi = pars['Jes'], pars['Jsi']
    #Jiq: float = pars['Jiq']  # 0.85; #nA
    Jin: float = pars['Jin']
    #N=20, sIn=.1, sOut=3., sEI=.2

    def __init__(self, sEI=.2, sIn=.1, sOut=3., N=30):  # sEI=4, sIn=.2, sOut=1.2,
        # Weights (from gaussian filter)
        self.N = N  # pars['NumN']
        self.wei = torch.tensor(dog_filter(sOut, int(N)), dtype=torch.float32)   # .astype( torch.float32))  # , dtype='float64'# fun.dLogGaus(.61, N)  #fun.dog_filter(sIn, sOut, N)#gaussian_filter(sEI, N)
        self.wii = torch.tensor(np.eye(int(N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)#np.eye(N) #
        self.wie = torch.tensor(gaussian_filter(sEI, int(N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)
        self.wes = torch.tensor(np.eye(int(N)), dtype=torch.float32)  #.astype(torch.float32))  # Identity matrix
        self.f = np.arange(1, N + 1)
        self.sEI = sEI
        self.sIn = sIn
        self.sOut = sOut

    def reset(self):  # https://stackoverflow.com/questions/56878667/setting-default-values-in-a-class

        for name, field in self.__dataclass_fields__.items():
            if field.default != MISSING:
                setattr(self, name, field.default)
            else:
                setattr(self, name, field.default_factory())


# °°° Time of the simulation °°°
class Simulation:
    def __init__(self, dt, T):
        self.dt = dt
        self.T = T
        self.range_t = (np.arange(0, self.T, self.dt))
        self.Lt = self.range_t.size

    def printSim(self):
        print("Time step of the simulation (dt):", self.dt, "  Duration of simulation (T):", self.T,"s",
              "  Length of the time frame (Lt):", self.Lt)


#  °°° Initialisation of the variables °°°
class Stim:
    def __init__(self, param, simu, f, ISI=0, dur=0.05):#ISI=0.5, dur=0.2): #ISI=1, dur=0.2   # 8 #[10]
        self.f = f  # array of frequency stimulus types
        self.ISI = ISI  # inter-stimulus interval
        self.dur = dur  # duration in s of a specific stimulus segment . The time the frequency fi ll be maintained in the f array
        self.tail = 0
        self.predDt = 0
        self.pred = 0
        self.InMax = param.InMax
        self.In0 = param.In0
        self.N = param.N

        # Instantaneous frequency
        #f_instant = np.zeros((int(self.ISI / simu.dt) + 1, 1))  # size ISI : 1 /dt : 1000

        for fx in self.f: #+2 to not lose dimension ! becareful! 
            fx_array = np.concatenate((np.ones((int(self.dur / simu.dt)+2, 1)) * fx,
                                       # just 1 frequency of 8 . # inter-stim interval is aslong as stim interval
                                       np.zeros((int(self.ISI / simu.dt),
                                                 1))))  # so I get 1 list with 1000 lists containing 8 and 1000 lists containing 0
    
        f_stim = fx_array # np.vstack((f_instant, fx_array))  # stack vertically these arrays # [0] *1000 , [8]*1000, [0]*1000
        self.f_stim = f_stim[1:]  # 1400*1
      
    # bottom up sensory Input # duration 1sec
    def sensoryInput(self, parameter, simu, sigmaIn=None, f_stim=None, InMax=None, In0=None):
        paramf = np.arange(1, self.N+1)
        
        #mask the ground truth f_stim by a gaussian function
        w = np.exp(-(((paramf) - (self.f_stim)) ** 2) / (
                2 * (sigmaIn or parameter.sigmaIn) ** 2))  # pars['f'] = 1:N
        #ic((w).shape)
        #if I want to normalize w:
        # totalAct = w.sum(axis = 1) #sum over each row
        # norm_w = (w.T / totalAct).T # elementwise division
        
        In = np.where(f_stim or self.f_stim > 0, (InMax or self.InMax) * w + (In0 or self.In0),
                      0)  # if stim >0 give InMax * weight + In0 otherwise give 0
        if self.tail != 0:
            tail_zeros = np.zeros((parameter.N +1, int(self.tail / simu.dt)))
            In = np.hstack((In, tail_zeros))
        
        In_short = In
        range_sim = np.arange(1, In_short.shape[0] + 1)
        len_sim = len(range_sim)
        self.In = In_short
        self.w = w
        self.sigmaIn = sigmaIn

        return In_short, range_sim, len_sim ,w, sigmaIn

    def printStim(self):
        print("frequence of stimulus f:", self.f, "  ISI:", self.ISI,"s","  Size In", self.In.shape, "Size w:",
              self.w.shape, "  f_stim = total length simulation:", self.f_stim.shape,
              "sigmaIn:", self.sigmaIn)


#  °°° Data preparation °°°       
""" 
1- create a big dataset for every stimuli input. In = X stimulus
2- Split In / X into train and test dataset: Split 70% into train dataset and 30% into test dataset
3- get expected Y / Target
4- Create the DataLoader merging Xtrain_tensor and Ytrain_tensor (Input and Target)
 """
class Batch:
    def __init__(self, param, simu,len_sim):
     # 1  \\\\\\\\\\\ BIG Bottom up sensory input
        self.N_short = param.N # - 6
        self.IN= torch.zeros(len_sim, self.N_short ,self.N_short)
        self.get_sensory_input(param,simu)
        

    def get_sensory_input(self, param, simu):
        for i in range(0, self.N_short):
            index = i+1
            st = Stim(param,simu, dur=simu.T,f =[index], ISI=0) 
            In, _,_,_,_ =st.sensoryInput(param, simu, sigmaIn = 2.)
            self.IN[:,:, i] = torch.tensor(In)
            #sti = torch.tensor(In).float()

    # 2
    def train_test_dataset(self):
        #create a random list containing each of our stimuli types
        num_stimuli = self.IN.shape[2]
        rand_idx = np.arange(0, num_stimuli)
        rng = np.random.default_rng(1245)
        rng.shuffle(rand_idx)

        # split this random list into test and train index. and filter the IN with those indexes
        val_split_index = int(np.floor(0.7 * num_stimuli))
        test_idx, train_idx = rand_idx[val_split_index:], rand_idx[:val_split_index]
        
        train_IN = self.IN[:,:, train_idx].permute(2,0,1)
        test_IN = self.IN[:,:, test_idx].permute(2,0,1)

        return train_IN, test_IN

    # 3
    def get_Targets(self, Inputs):
        Targets = torch.zeros_like(Inputs)
        num_stimuli, _,_ = Inputs.shape
        for stim_idx in range(num_stimuli):
            Targets[stim_idx,:,:] = get_expected_Y_relu(Inputs[stim_idx,:,:])
        return Targets

    # 4
    def create_dataloader(self, Inputs, Targets):   
        dataset = TensorDataset(Inputs, Targets)
        return DataLoader(dataset, batch_size=1, shuffle = True) #one sample per batch

    # 5
    def preprocess_data(self):
        train_IN, test_IN = self.train_test_dataset()
        #get expected target for every stimuli type /batch
        train_Targets = self.get_Targets(train_IN) 
        test_Targets = self.get_Targets(test_IN) 

        train_dataloader = self.create_dataloader(train_IN, train_Targets)
        test_dataloader = self.create_dataloader(test_IN, test_Targets)
        return train_dataloader, train_Targets, train_IN, test_dataloader, test_Targets, test_IN
    
    #6
    def safety_plot(self, train_IN, train_Targets):  
        X_proba = make_it_proba(train_IN[4,:,:])
        Y = train_Targets[4,:,:]
        N = X_proba.shape[1]      
        t=48
        plt.plot(torch.arange(1, N+1), X_proba[t,:], label= f"Proba Input at t={t}") #not in proba
        plt.plot(torch.arange(1, N+1), Y.detach().numpy()[t,:], label = f"target at t={t}") #in proba
        plt.legend()

class Batch_for_NLLL:
    def __init__(self, IN):
        self.Input =  IN
        BatchxTime = self.Input.shape[0]*  self.Input.shape[1]
        self.Target_index = torch.zeros(BatchxTime)
        self.make_Target_index(self.Input)
        
        self.Input_reshaped = self.Input.reshape(self.Input.shape[0]*  self.Input.shape[1], self.Input.shape[2])
    
    def make_Target_index(self, Input):
        time_stim = 0
        for batch in range(0, Input.shape[0]):
            for time in range(0, Input.shape[1]):
                self.Target_index[time + time_stim] = get_stimuli_input(Input[batch,:,:]).item()
            time_stim += Input.shape[1]


# °°° Stopping criterion for the loss °°°
class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_loss = float('inf')

    def early_stop(self, loss):
        if loss < self.min_loss:
            self.min_loss = loss
            self.counter = 0
        elif loss > (self.min_loss + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                print("--- loss is not improving - training will stop ---")
                return True
        return False
        
"""
*
*
***********  CLASS MYMODEL
*
*
"""
class Model(nn.Module):
    def __init__(self, param, sim, dicJ, In):
        super(Model, self).__init__()

        #--- Define other model parameters, layers, or components here if needed
        self.dt = sim.dt #torch.tensor(1e-4) #
        self.N = In.shape[1]#param.N - 6 #20
        self.taue = self.taui = torch.tensor(param.taue) #torch.tensor(0.005)
         # ¤ parameter of the phi function Not tweakable parameters
        self.ae = torch.tensor(param.ae)# 18.26)  # 2 #Wong have to check # Modelling and Meg Gain of the E populaiton
        self.be = torch.tensor(param.be) #-5.38)  # Threshold of the E populaiton
        self.hme = torch.tensor(param.hme)#78.67)
        self.ai = torch.tensor(param.ai)#21.97)
        self.bi = torch.tensor(param.bi)#-4.81)
        self.hmi = torch.tensor(param.hmi)#125.62)
        #create the smallest possible number
        self.epsilon = sys.float_info.epsilon

        self.sIn = torch.tensor(.1)
        self.sOut= 3.
        self.sEI = .2
        self.tauAMPA = torch.tensor(0.002)
        self.tauGABA = torch.tensor(0.005)


        """with open(file_name, "rb") as file:
            loaded_wee_wii = pickle.load(file)"""
        #self.Inoise = np.random.randn()
        #self.I_noise_E = torch.randn(size= (self.N,), dtype=torch.float32) #* 0.02 # if I want to change sigma #.astype(torch.float32)#(.2, .2, 1) #(0, .02, 1)
        #self.I_noise_E = torch.tensor(init_random_Tgaussian(self.N, mu =0., sigma = 1/np.sqrt(self.N)), dtype = torch.float32) #sigma = 1/np.sqrt(N)  
        self.Se_noise = torch.tensor(init_random_Tgaussian(self.N, mu =0., sigma = 1/np.sqrt(self.N)), requires_grad = False, dtype = torch.float32)#sigma = 1/np.sqrt(N)  
        self.Si_noise = torch.tensor(init_random_Tgaussian(self.N, mu =0., sigma = 1/np.sqrt(self.N)), requires_grad = False,dtype = torch.float32)#sigma = 1/np.sqrt(N)  

        #self.I_noise_I = torch.randn(0, .02, 1)

        self.wii =  torch.tensor(np.eye(int(self.N)), dtype=torch.float32) # self.wii =torch.tensor(loaded_wee_wii.detach().numpy() , dtype=torch.float32) #!!! N = 30  ## dog_filter(sIn, sOut, N)#np.eye(N) #
        self.wee = torch.tensor(np.eye(int(self.N)), dtype=torch.float32) #self.wee = torch.tensor(loaded_wee_wii.detach().numpy() , dtype=torch.float32) #

        #wee_o = init_random_matrix(N=self.N)
        #self.wee = nn.Parameter(torch.tensor(wee_o, requires_grad = True, dtype = torch.float32)) 
        #self.wii = nn.Parameter(torch.tensor(wii_o, requires_grad = True, dtype = torch.float32)) 

        # Example usage for w0 initialization 
        #self.wie = torch.tensor(gaussian_filter(self.sEI, int(self.N)), dtype=torch.float32) #.astype(torch.float32))  # dog_filter(sIn, sOut, N)
        #self.wei = torch.tensor(dog_filter(self.sOut, int(self.N)), dtype=torch.float32)         
        wei_o = init_random_matrix(N=self.N)
        wie_o = init_random_matrix(N=self.N)
        
        self.wes = torch.tensor(np.eye(int(self.N)), dtype=torch.float32)  # Identity matrix


        # initial parameters
        self.wei = nn.Parameter(torch.tensor(wei_o, requires_grad = True, dtype = torch.float32))  
        self.wie = nn.Parameter(torch.tensor(wie_o, requires_grad = True, dtype = torch.float32))

        self.dicJ = dicJ #kwargs
        
        self.Jee = torch.tensor(self.dicJ['Jee'], requires_grad = False, dtype=torch.float64)#0.2609) #nn.Parameter(torch.tensor(self.dicJ['Jee'], requires_grad = True, dtype=torch.float64)) #0.2609 nA, wong and wang
        self.Jei = nn.Parameter(torch.tensor(self.dicJ['Jei'], requires_grad = True, dtype=torch.float64))
        self.Jie = nn.Parameter(torch.tensor(self.dicJ['Jie'], requires_grad = True, dtype=torch.float64))
        self.Jii = nn.Parameter(torch.tensor(self.dicJ['Jii'], requires_grad = True, dtype=torch.float64))
        self.Jin = nn.Parameter(torch.tensor(self.dicJ['Jin'], requires_grad = True, dtype=torch.float64))

        #--- Initialize model variables here
    def initiate_state(self):
        self.prev_r_e = torch.zeros((self.N)) # torch.ones(self.N) shows more obvious results
        self.prev_r_i = torch.zeros((self.N))
        self.prev_s_ampa = torch.zeros((self.N))
        self.prev_s_gaba = torch.zeros((self.N))
        self.dr_e_dt = torch.zeros((self.N))
        self.dr_i_dt = torch.zeros((self.N))
        self.s_ampa = torch.tensor(0.)
        self.i_tot_e = torch.tensor(0.)
        self.i_tot_i = torch.tensor(0.)
		

    def phi(self, I_tot, a, b, hm): #)))  # this use a lot of memory - exponential part
      
        if (torch.isnan(I_tot).any())== True:
                    np.savetxt('NaNinput.txt', I_tot.detach().numpy())
                    #for param_tensor in model.state_dict():
                    #print(param_tensor, "\t", model.state_dict()[param_tensor].size())
                    #optimizer.state_dict()['params']
                    inhomo = model_new.state_dict() #.values()
                    homogeneous_array = [str(value) for value in inhomo.items()]
                    L2 = np.array(list(homogeneous_array))
                    np.savetxt('param.txt', L2, delimiter=" ", fmt='%s')# model.state_dict())

                    gradients_dict = {}

                    # Access gradients
                    for name, param in model_new.named_parameters():
                         gradients_dict[name] = param.grad
                    #homogeneous_array = [str(value) for value in inhomo.items()]
                    L3 = np.array(list(gradients_dict.items()))
                    #ic(L3) does not work
                    #np.savetxt('grad_param.txt', L3)#, delimiter=" ", fmt='%s')# model.state_dict())
                    #ic(I_tot)
                    quit()
                    sys.exit() #sys.
        mulaI = a * I_tot
        addB = mulaI + b
        expo = torch.exp(-addB)
        return hm / (1 + expo)
    #the operation Jee_re = self.Jee * prev_r_e => triggers inplace error  

    def forward(self, In):
        #--- Compute values of interest
   
        s_gaba_wie = self.prev_s_gaba @ self.wie
        s_ampa_wei = self.prev_s_ampa @ self.wei
        s_gaba_wii = self.prev_s_gaba @ self.wii
        s_ampa_wee = self.prev_s_ampa @ self.wee
      
        self.i_tot_e = (self.Jee * s_ampa_wee) - (self.Jie * s_gaba_wie) + (self.Jin* (In)) #+ self.I_noise_E
        self.i_tot_i = (self.Jei * s_ampa_wei) - (self.Jii * s_gaba_wii)

        phi_arr_e = self.phi(self.i_tot_e, self.ae, self.be, self.hme)
        phi_arr_i = self.phi(self.i_tot_i, self.ai, self.bi, self.hmi)

        self.dr_e_dt = ((-self.prev_r_e + phi_arr_e) / self.taue)
        self.dr_i_dt = ((-self.prev_r_i + phi_arr_i) / self.taui)

        r_e = self.prev_r_e+ self.dr_e_dt * self.dt
        r_i = self.prev_r_i + self.dr_i_dt * self.dt

        dS_amp_dt = (- self.prev_s_ampa / self.tauAMPA) + r_e + self.Se_noise
        s_ampa = self.prev_s_ampa+ dS_amp_dt * self.dt

        dS_gab_dt = (- self.prev_s_gaba / self.tauGABA) + r_i + self.Si_noise
        s_gaba = self.prev_s_gaba + dS_gab_dt * self.dt

        self.prev_r_e = r_e
        self.prev_r_i = r_i
        self.prev_s_ampa = s_ampa
        self.prev_s_gaba = s_gaba


        return self.prev_r_e, self.prev_r_i, self.dr_e_dt, self.dr_i_dt, self.prev_s_ampa, self.prev_s_gaba
    


    # \\\\ Parameters


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN forward pass and Print heatmap ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#model1 = Model(param=param, sim=sim, dicJ=J1, In = In)
#P0, I0, dP0dt = run_model_across_batch(Input_NLLL, len_sim, model1)

#HeatMap(P0[250:660,:].detach().numpy(), I0.detach().numpy(), J1)
#HeatMap(Input_NLLL.detach().numpy(), Input_NLLL.detach().numpy())

# here call the inner functions: self.decoding_loss(...)
#self.total_custom_loss(Prediction, Target, derivativeE)

def decoding_loss(Target_idx_np, lagrangian_multipliers, Prediction, Target, derivativeE, offset):
    criterion = torch.nn.NLLLoss()
    #tot_P =Prediction + noisy_RE

    P2 = make_it_proba(Prediction, offset)
    loss_norm = criterion(P2, Target.long())    

    loss_derivative = torch.zeros_like(Target)     #loss_derivative.fill_(float('nan'))
    #len_sim_test = len_sim
    
    count = 0
    #count += 1
    start = 0
    stop = (len_sim)
    for i, sti_idx in enumerate(Target_idx_np):
        count += 1
        if count == stop:
            loss_derivative[i] = - (F.softplus(derivativeE[start:stop,sti_idx] ).sum() * 4) + F.softplus(derivativeE[start:stop,:sti_idx]).sum() + F.softplus(derivativeE[start:stop,(sti_idx+1):]).sum()
            #loss_derivative[i2] = - F.softplus(derivativeE[time_idx,sti_idx] ).mean() + F.softplus(derivativeE[time_idx,:sti_idx]).mean() + F.softplus(derivativeE[time_idx,(sti_idx+1):]).mean()
            start = stop
            stop += len_sim
    """"""
    loss_derivative_term = loss_derivative.mean()
    l_norm = loss_norm * lagrangian_multipliers['normalization']
    l_der = loss_derivative_term * lagrangian_multipliers['derivative']
    return l_norm, l_der


def control_system(Target_idx_np, parm, lagrangian_multipliers, eps, Prediction):
        #stable and bounded system

    hyperactvity_penalty = torch.zeros(train_IN.shape[0])
    laziness_penalty = torch.zeros(train_IN.shape[0])
    """
    count = 0
    i2 = 0 # pitch type number
    start = 0
    stop = (len_sim)

    for i, sti_idx in enumerate(Target_idx_np):
        count +=1
        if count == stop: #count == stop:
            time_idx = slice(start, stop)  
            
            #sti_idx
                #hyperactvity_penalty[i2] = ((Prediction[time_idx, sti_idx]**2)).sum()
            #hyperactvity_penalty[i2] = ((Prediction[time_idx, :]**2)).sum()
            
            if torch.max(Prediction[time_idx,:]) > 60.: 
            else:
                hyperactvity_penalty[i2] = 0
            if torch.max(Prediction[time_idx,sti_idx]) < 40. : #40.: #aLEJANDRO SUGGEST. 40.hz #10 #20 works ok
                laziness_penalty[i2] = torch.sum(1 / torch.clamp(Prediction[time_idx, sti_idx]**2, min = eps)) #torch.clamp(r_e[:, 7], max=10.0
            else:
                laziness_penalty[i2] = 0
               
            i2 += 1
            start = stop
            stop += len_sim """
            
    #negatif_param = []
    
    #4/ Constraint ws and Js        
    """be_positive = torch.zeros(1)
    param_l = list(self.parm.items())
    for i in range(len(param_l)):
        be_positive = sum_relu(param_l[i][1]) + be_positive #torch.sum(F.relu(-wie)) +torch.sum(F.relu(- wei)) + torch.sum(F.relu(- win))#""""""
    """
    be_positive = torch.zeros(1) 
    neg_param_values = [torch.sum(F.relu(-values[values<0])) for values in parm.values() if torch.any(values < 0)]
    if neg_param_values:
        be_positive = torch.sum(torch.stack(neg_param_values))
        #ic(be_positive)
    be_p = be_positive * lagrangian_multipliers['be_positive']
    
    hyper = hyperactvity_penalty.mean() * lagrangian_multipliers['hyperactivity']
    lazi = laziness_penalty.sum() * lagrangian_multipliers['laziness'] 
    return hyper, lazi, be_p#hyperactvity_penalty.mean() , laziness_penalty.sum() , be_positive


def activity_efficiency(re,ri, lagrangian_multipliers):
    #more selective use of neurons in representing information.
    tau_s = 1#0.001
    tau_i = 5
    sparse_coding_wei = lagrangian_multipliers['sparse_coding'] * torch.sum(torch.log1p((re**2 / tau_s)))#log1p = log(1+ (x))#torch.sum(torch.abs(W["wei"])) #)
    sparse_coding_wie = lagrangian_multipliers['sparse_coding'] * torch.sum(torch.log1p((ri**2 / tau_i))) #torch.sum(torch.abs(W["wie"]))#
    sparse_coding_term = (sparse_coding_wei + sparse_coding_wie) 
    return sparse_coding_term  


def architecture_efficiency(parm, lagrangian_multipliers,N):
    distances = torch.from_numpy(logistic_func_dd(N, 1))
    #distances = torch.from_numpy(np.power(np.abs(np.arange(N)[np.newaxis, :] - np.arange(N)[:, np.newaxis]),1.5)) # raised tot the power 2 . cost_distance_inh = torch.sum(parm["wie"] * distances) 
    cost_distance_inh = torch.sum(parm["wie"] * distances) 
    distance_inh = cost_distance_inh * lagrangian_multipliers['distance_diag']
    return distance_inh 

#@jit(target_backend='cuda') 

def total_custom_loss(model_t, Prediction, I, Target, derivativeE, lagrangian_multipliers, N, eps, offset): #, model !!!
    #noisy_RE = torch.tensor(init_random_Tgaussian(N), dtype = torch.float32)
    
    Target_idx_np = Target.numpy().astype(int)
    parm = {name: par for name, par in model_t.named_parameters()}
    #ic(parm.keys())
    #for name, par in model_t.named_parameters() : # enumerate(trained_model.parameters()):
     #       parm[name] = par 
    _norm, _derivative = decoding_loss(Target_idx_np, lagrangian_multipliers,Prediction, Target, derivativeE, offset)#_derivative
    hyper_coef, lazy_coef, positive_param = control_system(Target_idx_np, parm, lagrangian_multipliers, eps, Prediction)
    sparse_cod = activity_efficiency(Prediction,I, lagrangian_multipliers)
    cost_dis_inh = architecture_efficiency(parm, lagrangian_multipliers, N)
        
    Cost = _norm  + cost_dis_inh + hyper_coef + lazy_coef + sparse_cod + positive_param+ _derivative #* self.lagrangian_multipliers['sparse_coding']
    #ic(_norm, _derivative, cost_dis_inh, hyper_coef, lazy_coef, sparse_cod, positive_param)
    #ic(Cost)
    return Cost

#@jit(target_backend='cuda') 
class run_optimization:
    def __init__(self, model, opti_name = "Adam" ,num_epoch =20, learning_rate =0.001, sparse_coef = 3E-2, dist_diag = 1E-1):
        self.N = param.N
        self.eps = torch.FloatTensor([sys.float_info.epsilon]) 
    
        self.lagrandian_mul = {'normalization': 7000,'derivative': 1E-6, 'hyperactivity':1E-8, 'laziness': 0.01,

                                    'distance_diag': dist_diag, 'sparse_coding':sparse_coef, 'be_positive': 200}
        #distance_diag = 1E-1

        #self.criterion = torch.nn.NLLLoss()
        #self.logsoft = nn.LogSoftmax(dim=1)
        self.W = {}
        self.early_stopper = EarlyStopper(patience=15, min_delta=2)#3

        self.losses = np.zeros(num_epoch)# torch.zeros(num_epoch)

        if opti_name =="Adam":
            self.optimizer = optim.Adam(model.parameters(),
                      lr=learning_rate)
        elif opti_name == "SGD":
            self.optimizer = optim.SGD(model.parameters(),
                      lr=learning_rate, weight_decay = 0.001)
        
        #self.run(criterion, logsoft, model, Input, T, modelpath, num_epoch,learning_rate)
    
    #@jit(target_backend='cuda') 
    def run(self, model, Input, T, modelpath, num_epoch, learning_rate,offset): 
        model.initiate_state()
        model.train()
        for epoch in range(num_epoch):
            model.initiate_state()
            self.optimizer.zero_grad()  
            P = torch.zeros(Input.shape)   
            I = torch.zeros(Input.shape) 
            dPdt = torch.zeros_like(Input)  

            count = 0
            for i, b in enumerate(T):
                count +=1
                P[i,:], I[i,:], dPdt[i,:], dridt, ampa, gaba = model(Input[int(i),:])
                if count == len_sim: #train_IN.shape[1]: #if we end the simulation time and go to the next batch
                    count = 0
                    model.initiate_state()

            #P2 = logsoft(P)
            #P2 = make_it_proba(P)
            #loss = criterion(P2, T.long())  #loss = criterion(P, T.long()) # #criterion(P2, T.long()) # #loss = easyLoss(P2, train_Targets_reshaped) #loss = custom_loss_(P, Target_idx_NLLL, dPdt)
            #loss_c = custom_loss(T)
            #loss = loss_c.total_custom_loss(P, T, dPdt)
            #loss = custom_loss_(model, P, I, T, dPdt) # 
            loss = total_custom_loss(model, P, I, T, dPdt, self.lagrandian_mul, self.N, self.eps, offset)
            loss.backward()
            self.optimizer.step()
            
            #ic(torch.max(P))#, torch.min(P),torch.mean(P))
            
            if self.early_stopper.early_stop(loss):  
                break
            

            self.losses[epoch] = loss
            print(f'Epoch [{epoch + 1}/{num_epoch}], Loss: {loss}') # , Learning rate:{learning_rate}')#.item()
            """for par in model.parameters():
                ic(par.grad)"""




        for name, par in model.named_parameters() : # enumerate(trained_model.parameters()):
            self.W[name] = par #getattr(trained_model, "wee)
            if name.startswith("J"):
                self.W.popitem()
                break


        count_neg = 0
        count_zero = 0
        for i in self.W['wie']:
            for wie in i:
                if wie < 0:
                    count_neg += 1
                    #ic(wie)
                if wie == 0:
                    count_zero += 1

        ic(count_neg, count_zero)
    
        """file_name_wie = f"WIE_sp{sparse}_dd{distance_D}_norm{norm}.txt"#str(rand_num_fig)
        file_name_wei = f"WEI_sp{sparse}_dd{distance_D}_norm{norm}.txt"#str(rand_num_fig)
        with open(file_name_wie, "w") as fp:
            json.dump(self.W['wie'], fp)  # encode dict into JSON
        #print(f"lagrandian_multiplier_sp{sparse}_dd{distance_D} into .txt file")
        with open(file_name_wei, "w") as fp:
            json.dump(self.W['wei'], fp)  # encode dict into JSON """
        
        rand_num_fig = np.random.randint(0, 999)
        sparse = self.lagrandian_mul['sparse_coding']
        distance_D = self.lagrandian_mul['distance_diag']
        norm = self.lagrandian_mul['normalization']

        #Save variable to file using pickle
        wei_saved = self.W['wei']
        wie_saved = self.W['wie']
        file_name_wie = f"WIE_sp{sparse}_dd{distance_D}_norm{norm}_{str(rand_num_fig)}.pkl"#
        file_name_wei = f"WEI_sp{sparse}_dd{distance_D}_norm{norm}_{str(rand_num_fig)}.pkl"#str(rand_num_fig)
        with open(file_name_wie, "wb") as file:
            pickle.dump(wie_saved, file)
        with open(file_name_wei, "wb") as file:
            pickle.dump(wei_saved, file)
       
        
        
        #save init param to file using pickle
        """file_name = f"lagrandian_multiplier_{str(rand_num_fig)}.pkl"
        with open(file_name, "wb") as file:
            pickle.dump(self.lagrandian_mul, file)
        """

        
        file_name = f"lagrandian_multiplier_sp{sparse}_dd{distance_D}_norm{norm}.txt"#str(rand_num_fig)
        with open(file_name, "w") as fp:
            json.dump(self.lagrandian_mul, fp)  # encode dict into JSON
        print(f"lagrandian_multiplier_sp{sparse}_dd{distance_D} into {rand_num_fig}.txt file")
        print(f"Offset for the make it proba is{offset}")
        # plot     
        r = np.round(np.random.rand(),1)
        g = np.round(np.random.rand(),1)
        b = np.round(np.random.rand(),1)
        color_l = [r,g,b]
        self.color_list = color_l
        #ic(model.Jee.numpy())
        plt.plot(np.arange(0,50), P[:50,6].detach().numpy(), label = f"pop={6 + 1}, exc")
        plt.plot(np.arange(0,50), I[:50,6].detach().numpy(), label = f"pop={6 +1}, inh")
        plt.plot(np.arange(0,50), P[:50,5].detach().numpy(), label = f"pop={5+ 1}, exc")
        plt.plot(np.arange(0,50), I[:50,5].detach().numpy(), label = f"pop={5 +1}, inh")
        plt.legend()
        plt.title("Firing rate over time after training, when pitch value = 7")
        plt.show()
        J2= {'Jee': model.Jee.numpy(), 'Jei': model.Jei.detach().numpy(), 'Jie': model.Jie.detach().numpy(), 'Jii': model.Jii.detach().numpy(), 'Jin': model.Jin.detach().numpy()}
        #ic(J2)
        fig0, (axA, axB)  = HeatMap(P[:51,:].detach().numpy(), I[:50,:].detach().numpy(), J2, toshow = False)
        fig0.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/OutputNetwork/Heatmap/Heatmap_Pred_{learning_rate}_{num_epoch}_sp{sparse}__dd{distance_D}_{rand_num_fig:0>3}.png')

        fig1 = plt.figure(tight_layout=True)
        gs = gridspec.GridSpec(1,2, figure = fig1)
        ax1 = fig1.add_subplot(gs[0,0])
        ax1.plot(np.arange(self.losses.shape[0]), self.losses, 'bo', color = color_l, label = f'learning rate = {learning_rate}')#.detach().numpy()
        ax1.set_title(f"Training Loss over {num_epoch} epochs for a lr of {learning_rate}, sp:{sparse}, dd:{distance_D}")

        ax2 = fig1.add_subplot(gs[0,1])
        ax2.plot(np.arange(P.shape[1]), P[50,:].detach().numpy(), color = color_l, label = f"dd={distance_D}, sp={sparse}, offset={offset}")
        ax2.set_title( f'Firing rate prediction after training over {num_epoch} epochs, and lr={learning_rate}') #ax.set_ylabel('YLabel1 %d' % i)#ax.set_xlabel('XLabel1 %d' % i)

        fig1.set_figheight(5)
        fig1.set_figwidth(10)#10
        fig1.align_labels()  # same as fig.align_xlabels(); fig.align_ylabels()
        plt.legend()
        plt.show()


        #ADD HEATPLOTHERE
        fig1.savefig(f'C:/Users/knzga/Documents/02_Computational Neuroscience Project/Image/OutputNetwork/Pred_losses_{learning_rate}_{num_epoch}_sp{sparse}__dd{distance_D}_{rand_num_fig:0>3}.png')
        #save_weights(model, "Optimized.pth")
        save_weights(model, modelpath)
        connectivity_matrix_2x2(model, Learning_Rate =learning_rate , n_epoch =num_epoch, sparce = sparse, distance_diag = distance_D, rand = rand_num_fig)    
        plt.show()

        #plot_connectivity_matrix(model, rand_num_fig, Learning_Rate= learning_rate, n_epoch= num_epoch, sparce= sparse, distance_diag = distance_D, rand = rand_num_fig)
        #plt.show()                





if __name__=="__main__":
    param = Parameter(N = 40)#30 # N=20
    # \\\\ Simulation time: T in s  (2s before)
    sim = Simulation(dt=1e-3,T=.0510) 
    #sim.printSim()
    # \\\\ Bottom up sensory input
    stimuli = Stim(param, sim, dur=sim.T, f=[8])
    In, range_sim, len_sim, w, sigmaIn = stimuli.sensoryInput(param, sim, sigmaIn=2.) #2.
    #stimuli.printStim()


    batch_instance = Batch(param=param, simu=sim, len_sim = sim.Lt)
    train_dataloader, train_Targets, train_IN, test_dataloader, test_Targets, test_IN = batch_instance.preprocess_data()
    #batch_instance.safety_plot(train_IN,train_Targets)
    #batch_instance.IN[:,3:-3,3:-3].shape

    batch_instance_NLLL = Batch_for_NLLL(train_IN)
    Target_idx_NLLL = batch_instance_NLLL.Target_index
    Input_NLLL =batch_instance_NLLL.Input_reshaped


    # +++++++++++++++++++++++++ Initialize the Model ++++++++++++++++++++++++++++
    J1 = {'Jee': 0.2609, 'Jei': 0.004, 'Jie': 0.05, 'Jii': 0.6, 'Jin': 0.00695} #old Jee: 0.072
    J_list = list(J1.keys())
    print(J1['Jee'], J_list)


    num_epoch = 600 #2000# 1600#good good 1500#1700# 1500


    #Epoch=[800, 1200, 1500, 1800]


    Sparse_coefs = [1E-3]#1E-2 good,[510E-2]#[610E-2]#[510E-2]#good[610E-2]#[510E-2] # [210E-2,110E-2,910E-3,860E-3,810E-3] #[800E-3, 770E-3,760E-3, 755E-3,750E-3] #610E-3 # 750E-3 # 610E-3            #395E-3#200E-3 #140E-3#120E-3#115E-3
    sparse = 1E-3 #2E-5#5E-5 #good5E-3#1E-3 #510E-2#510E-2##610E-2#510E-2#
    optim_list = []
    #distance_diag = [300E-2]#good good[300E-2]#good[300E-2]#[300E-2, 315E-2]# , 185E-2,180E-2]#[180E-2]#, 190E-2,180E-2, 160E-2] #180E-2 good, 100E-2]#[100E-2] #[30E-2]#10E-2 #31E-2, 31E-2, 45E-2, 40E-2[15E-2]    #[25E-2] #[35E-2] #[25E-2]#[15E-2]           #[12E-2, 15E-2, 20E-2, 40E-2]#1E-1 initial
    #distance_diag = [100E-2,300E-2, 400E-2, 800E-2]# [500E-2]#[500E-2]    
    distance_diag = [300E-2]#[2E-2]#,10E-2,40E-2, 60E-2, 200E-2
    dist = 300E-2#[15E-2]    #[25E-2] #[35E-2] #[25E-2]#[15E-2]           #[12E-2, 15E-2, 20E-2, 40E-2]#1E-1 initial
    #lr = 1E-3#2E-3#1E-2
    lr = 2E-3 #good    #  5E-4
    #lr = 2E-4
    #LR = [2E-3]#[1E-3]#[5E-3] #[2E-3] 
    #not good 3E-4#2E-3#18E-4#2E-3#not good18E-4#2E-3 goof good#22E-4 #22E-4# 25E-4 #[2E-3]
    Offset = [1]#[.5]good with 500epochs [1.2] good with 600epochs  #[1.8]#[1.15,1.3,1.5,1.6,1.8]

    #Sparse_coefs = [115E-3]
    #for i, num_epoch in enumerate(Epoch):
    #for i, dist in enumerate(distance_diag):
    #for i, sparse in enumerate(Sparse_coefs):
    #for i, lr in enumerate(LR):
    for i, off in enumerate(Offset):
        model_new = Model(param,sim, J1, Input_NLLL)
        result = run_optimization(model_new, opti_name = "Adam" ,num_epoch = num_epoch, learning_rate = lr, sparse_coef = sparse, dist_diag= dist)
        start = timer()
        ic(lr, sparse)
        result.run(model_new, Input_NLLL, Target_idx_NLLL, modelpath= "Optimized_good_lr_" + f"{lr}" + f"{sparse}"  , num_epoch=num_epoch, learning_rate=lr, offset = off)
        print("without GPU:", (timer()-start)/ 60, " minutes") 
        optim_list.append(result)
        print(result)








"""
    W = {}
    for name, par in model_new.named_parameters() : # enumerate(trained_model.parameters()):
        W[name] = par #getattr(trained_model, "wee)
        if name.startswith("J"):
            W.popitem()
            break

    count_neg = 0
    for i in W['wie']:
        for wie in i:
            if wie < 0:
                count_neg += 1
                ic(wie)

    ic(count_neg)
    print(W['wie'].shape)


plt.matshow(W['wei'].detach().numpy(), cmap= cmap2)
plt.show()
plt.plot(np.arange(param.N), W['wei'][15,:].detach().numpy() )
plt.show()
"""
#plt.plot(np.arange(P[:51,:].detach().numpy().shape[0]), P[:51,6].detach().numpy(), label = "pop number 7")
#plt.legend()
